{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report,f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration full-c8d896e19c9bb48e\n",
      "Reusing dataset ik_nlp22_pe_style (C:\\Users\\amit\\.cache\\huggingface\\datasets\\GroNLP___ik_nlp22_pe_style\\full-c8d896e19c9bb48e\\1.0.0\\3bbf0fda4806257149c2beb42c6cd20db6f79dac9ae2498f44be55fa7a953d51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1ccaaee2fc74c17a07ff149a2849fa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataDir='dataset/IK_NLP_22_PESTYLE'\n",
    "dataset = load_dataset(\"GroNLP/ik-nlp-22_pestyle\", \"full\", data_dir=dataDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (1170, 30)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1170, 30)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_whole_dataset = dataset[\"train\"]\n",
    "new_df= pd.DataFrame(data=training_whole_dataset)\n",
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (3.2.4)\n",
      "Requirement already satisfied: transformers in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (4.18.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (0.1.96)\n",
      "Requirement already satisfied: datasets in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (1.3.5)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy) (2.4.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy) (4.63.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: click<8.1.0 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy) (8.0.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\amit\\appdata\\roaming\\python\\python37\\site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy) (58.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy) (0.7.7)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy) (0.9.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy) (1.0.6)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy) (0.6.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy) (3.0.3)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy) (3.10.0.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy) (8.0.15)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\amit\\appdata\\roaming\\python\\python37\\site-packages (from spacy) (1.21.5)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\amit\\appdata\\roaming\\python\\python37\\site-packages (from transformers) (4.11.3)\n",
      "Requirement already satisfied: sacremoses in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from transformers) (0.0.49)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from transformers) (0.11.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\amit\\appdata\\roaming\\python\\python37\\site-packages (from transformers) (2020.6.8)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: dill in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: multiprocess in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: xxhash in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from datasets) (2022.2.0)\n",
      "Requirement already satisfied: aiohttp in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: responses<0.19 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from datasets) (7.0.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\amit\\appdata\\roaming\\python\\python37\\site-packages (from scikit-learn) (0.15.1)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from scikit-learn) (1.7.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\amit\\appdata\\roaming\\python\\python37\\site-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.7.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from click<8.1.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\amit\\appdata\\roaming\\python\\python37\\site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\amit\\appdata\\roaming\\python\\python37\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\amit\\appdata\\roaming\\python\\python37\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\amit\\appdata\\roaming\\python\\python37\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: asynctest==0.13.0 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from aiohttp->datasets) (0.13.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from aiohttp->datasets) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting it-core-news-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/it_core_news_sm-3.2.0/it_core_news_sm-3.2.0-py3-none-any.whl (21.4 MB)\n",
      "Requirement already satisfied: en_core_web_sm in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from en_core_web_sm) (3.2.4)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en_core_web_sm) (0.4.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en_core_web_sm) (3.0.3)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en_core_web_sm) (3.10.0.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en_core_web_sm) (0.7.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en_core_web_sm) (2.0.6)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en_core_web_sm) (0.6.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en_core_web_sm) (21.3)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en_core_web_sm) (58.0.4)\n",
      "Requirement already satisfied: click<8.1.0 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en_core_web_sm) (8.0.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\amit\\appdata\\roaming\\python\\python37\\site-packages (from spacy<3.3.0,>=3.2.0->en_core_web_sm) (1.21.5)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en_core_web_sm) (8.0.15)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en_core_web_sm) (0.9.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en_core_web_sm) (2.0.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\amit\\appdata\\roaming\\python\\python37\\site-packages (from spacy<3.3.0,>=3.2.0->en_core_web_sm) (2.27.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en_core_web_sm) (1.8.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en_core_web_sm) (3.0.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en_core_web_sm) (3.0.9)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en_core_web_sm) (1.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en_core_web_sm) (1.0.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en_core_web_sm) (4.63.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en_core_web_sm) (2.4.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en_core_web_sm) (3.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\amit\\appdata\\roaming\\python\\python37\\site-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en_core_web_sm) (3.7.0)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\amit\\appdata\\roaming\\python\\python37\\site-packages (from click<8.1.0->spacy<3.3.0,>=3.2.0->en_core_web_sm) (4.11.3)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from click<8.1.0->spacy<3.3.0,>=3.2.0->en_core_web_sm) (0.4.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en_core_web_sm) (3.0.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en_core_web_sm) (5.2.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\amit\\appdata\\roaming\\python\\python37\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en_core_web_sm) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en_core_web_sm) (2021.10.8)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-10 00:25:55.461793: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-04-10 00:25:55.461832: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\amit\\appdata\\roaming\\python\\python37\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en_core_web_sm) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\amit\\appdata\\roaming\\python\\python37\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en_core_web_sm) (2.0.12)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en_core_web_sm) (2.1.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('it_core_news_sm')\n",
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from en-core-web-sm==3.2.0) (3.2.4)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\amit\\appdata\\roaming\\python\\python37\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.21.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: click<8.1.0 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.4)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.7)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.63.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.15)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (58.0.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\amit\\appdata\\roaming\\python\\python37\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.27.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.7)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\amit\\appdata\\roaming\\python\\python37\\site-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.7.0)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\amit\\appdata\\roaming\\python\\python37\\site-packages (from click<8.1.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.11.3)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from click<8.1.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\amit\\appdata\\roaming\\python\\python37\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\amit\\appdata\\roaming\\python\\python37\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\amit\\appdata\\roaming\\python\\python37\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-10 00:26:12.927185: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-04-10 00:26:12.927293: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\envs\\py37\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.1.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Installing all requirements \n",
    "!pip install spacy transformers sentencepiece datasets scikit-learn pandas\n",
    "\n",
    "#We first download the \"it_core_new_sm\" for italian language and  \"en_core_web_sm\" for English \n",
    "# to use in tokenizer in order to get tokens, POS taggs \n",
    "!python -m spacy download it_core_news_sm en_core_web_sm\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import spacy library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_name = \\'\\'#\"mrm8488/mbart-large-finetuned-opus-it-en-translation\" \\nidentifier=\"Helsinki-NLP/opus-mt-it-en\"\\n\\nfrom transformers import BartTokenizer\\ntokenizer = BartTokenizer.from_pretrained(identifier)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "#from spacy.lang.it.examples import sentences \n",
    "\"\"\"from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\"\"\"\n",
    "\"\"\"model_name = \"dbmdz/bert-base-italian-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name) #'bert-base-uncased')\"\"\"\n",
    "'bert-base-multilingual-cased'\n",
    "\"\"\"from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-italian-cased')\n",
    "model = BertModel.from_pretrained(\"dbmdz/bert-base-italian-cased\")\"\"\"\n",
    "\"\"\"import nltk\n",
    "import string\n",
    "nltk.download('punkt')\"\"\"\n",
    " \n",
    "# Define the model repo\n",
    "\"\"\"model_name = ''#\"mrm8488/mbart-large-finetuned-opus-it-en-translation\" \n",
    "identifier=\"Helsinki-NLP/opus-mt-it-en\"\n",
    "\n",
    "from transformers import BartTokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(identifier)\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing ...\n",
    "### Loading Spacy and models\n",
    "    We load both italian and english language models with spacy in order to use them for tokenizing and POS tagging purposes.\n",
    "### POS tags extraction functions\n",
    "    In this part of code, we defined some usefull function applying in the preprocessing operation to extract POS taggs from the input text src_text, tgt_text. Beacuse of mt_text masking in test_modality and test_edit datasets, we would not extract the mt_text.     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nlp = spacy.load('it_core_news_sm')\n",
    "nlpSrc = spacy.load('en_core_web_sm')\n",
    "#nlp = spacy.load('it_core_news_md')\n",
    "\n",
    "def clean_text(text):\n",
    "    doc = nlp(text)\n",
    "    print_attributes(doc)\n",
    "    return getCountOfPOStagging(doc)\n",
    "\n",
    "NOUN='NOUN'\n",
    "VERB='VERB'\n",
    "ADJ='ADJ'\n",
    "ADV='ADV'\n",
    "PROPN='PROPN'\n",
    "DET='DET'\n",
    "PRT='PRT'\n",
    "CONJ='CONJ'\n",
    "PUNCT='PUNCT'\n",
    "ADP='ADP'\n",
    "AUX='AUX'\n",
    "CCONJ='CCONJ'\n",
    "INTJ='INTJ'\n",
    "NUM='NUM'\n",
    "PART='PART'\n",
    "PRON='PRON'\n",
    "SCONJ='SCONJ'\n",
    "SYM='SYM'\n",
    "lst=[NOUN,VERB,ADJ,ADV,PROPN,PUNCT,ADP,AUX,CONJ,CCONJ,INTJ,NUM,PART,PRON,SCONJ,SYM]\n",
    "\n",
    "def checkWordsInVocabDictionary(text):\n",
    "    count=0\n",
    "    doc=nlp(text)\n",
    "    for token in doc:\n",
    "        tt = nlp.vocab.strings[token.text]\n",
    "        if tt in nlp.vocab:\n",
    "            count +=1\n",
    "    return count\n",
    "#for nlp(italian)\n",
    "def getNumberOfTokens(text):\n",
    "    count=0\n",
    "    doc=nlp(text)\n",
    "    for token in doc:\n",
    "        count +=1\n",
    "    return count\n",
    "   \n",
    "def getCount(text,lstPOS):\n",
    "    doc = nlp(text)\n",
    "    count=0\n",
    "    for token in doc:\n",
    "        if token.pos_ in lstPOS: count += 1 \n",
    "    return count   \n",
    "\n",
    "def countWords(text, threshold):\n",
    "    doc=nlp(text)\n",
    "    count=0\n",
    "    for token in doc:\n",
    "        if len(token.text) < threshold :\n",
    "            #print(token.text +\"\\t\"+token.pos_)\n",
    "            count +=1\n",
    "    return count        \n",
    "\n",
    "# for nlp(english)\n",
    "def getNumberOfTokens_Src(text):\n",
    "    count=0\n",
    "    doc=nlpSrc(text)\n",
    "    for token in doc:\n",
    "        count +=1\n",
    "    return count\n",
    "   \n",
    "def getCount_Src(text,lstPOS):\n",
    "    doc = nlpSrc(text)\n",
    "    count=0\n",
    "    for token in doc:\n",
    "        if token.pos_ in lstPOS: count += 1 \n",
    "    return count   \n",
    "\n",
    "# @param text : the input english text from src_text\n",
    "# @Param threshold: the number of 3 and 4 used to extract word with length less than 3 and 4. \n",
    "# Later we will seperate this words from the the set od DET and PUNCT words.\n",
    "# DET tag categories consists of determoners and articles \n",
    "def countWords_Src(text, threshold):\n",
    "    doc=nlpSrc(text)\n",
    "    count=0\n",
    "    for token in doc:\n",
    "        if len(token.text) < threshold :\n",
    "            count +=1\n",
    "    return count        \n",
    "\n",
    "#Test:\n",
    "#getCountPunc('I corpi di pace armati delle Nazioni Unite,')\n",
    "#I corpi di pace armati delle Nazioni Unite,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Preprocessing \n",
    "\n",
    "To have a better results, We need to clean the data and add some new feature. So we provided some functions for preprocessing.\n",
    "Function PreProcess is the main functin to form the train data.\n",
    "The train split contains a total of 1170 triplets (or pairs, when translation from scratch is performed) annotated with behavioral data produced during the translation. The followings are the fields and their descriptions of the train dataset:\n",
    "#### Train Set\n",
    "**Fields**\n",
    "\n",
    ">**item_id**\t=> The sentence identifier. The first digits of the number represent the document containing the sentence, while the last digit of the number represents the sentence position inside the document. Documents can contain from 3 to 5 semantically-related sentences each.\n",
    "\n",
    ">**subject_id**\t=> The identifier for the translator performing the translation from scratch or post-editing task. Values: t1, t2 or t3.\n",
    "\n",
    ">**modality**\t=> The modality of the translation task. Values: ht (translation from scratch), pe1 (post-editing Google Translate translations), pe2 (post-editing mBART translations).\n",
    "\n",
    "\n",
    ">**src_text**\t=> The original source sentence extracted from Wikinews, wikibooks or wikivoyage.\n",
    "\n",
    ">**mt_text**\t=> Missing if tasktype is ht. Otherwise, contains the automatically-translated sentence before post-editing.\n",
    "\n",
    ">**tgt_text**\t=> Final sentence produced by the translator (either via translation from scratch of sl_text or post-editing mt_text)\n",
    "\n",
    ">**edit_time**\t=> Total editing time for the translation in seconds.\n",
    "\n",
    ">**k_total**\t=> Total number of keystrokes for the translation.\n",
    "\n",
    ">**k_letter**\t=> Total number of letter keystrokes for the translation.\n",
    "\n",
    ">**k_digit**\t=> Total number of digit keystrokes for the translation.\n",
    "\n",
    ">**k_white**\t=> Total number of whitespace keystrokes for the translation.\n",
    "\n",
    ">**k_symbol**\t=> Total number of symbol (punctuation, etc.) keystrokes for the translation.\n",
    "\n",
    ">**k_nav**\t=> Total number of navigation keystrokes (left-right arrows, mouse clicks) for the translation.\n",
    "\n",
    ">**k_erase**\t=> Total number of erase keystrokes (backspace, cancel) for the translation.\n",
    "\n",
    ">**k_copy**\t=> Total number of copy (Ctrl + C) actions during the translation.\n",
    "\n",
    ">**k_cut**\t=> Total number of cut (Ctrl + X) actions during the translation.\n",
    "\n",
    ">**k_paste**\t=> Total number of paste (Ctrl + V) actions during the translation.\n",
    "\n",
    ">**n_pause_geq_300**\t=> Number of pauses of 300ms or more during the translation.\n",
    "\n",
    ">**len_pause_geq_300**\t=> Total duration of pauses of 300ms or more, in milliseconds.\n",
    "\n",
    ">**n_pause_geq_1000**\t=> Number of pauses of 1s or more during the translation.\n",
    "\n",
    ">**len_pause_geq_1000**\t=> Total duration of pauses of 1000ms or more, in milliseconds.\n",
    "\n",
    ">**num_annotations**\t=> Number of times the translator focused the texbox for performing the translation of the sentence during the translation session. E.g. 1 means the translation was performed once and never revised.\n",
    "\n",
    ">**n_insert**\t=> Number of post-editing insertions (empty for modality ht) computed using the tercom library.\n",
    "\n",
    ">**n_delete**\t=> Number of post-editing deletions (empty for modality ht) computed using the tercom library.\n",
    "\n",
    ">**n_substitute**    => Number of post-editing substitutions (empty for modality ht) computed using the tercom library.\n",
    "\n",
    ">**n_shift**\t=> Number of post-editing shifts (empty for modality ht) computed using the tercom library.\n",
    "\n",
    ">**bleu**\t=> Sentence-level BLEU score between MT and post-edited fields (empty for modality ht) computed using the SacreBLEU library with default parameters.\n",
    "\n",
    ">**chrf**\t=> Sentence-level chrF score between MT and post-edited fields (empty for modality ht) computed using the SacreBLEU library with default parameters.\n",
    "\n",
    ">**ter**\t=> Sentence-level TER score between MT and post-edited fields (empty for modality ht) computed using the tercom library.\n",
    "\n",
    ">**aligned_edit**\t=> Aligned visual representation of REF (mt_text), HYP (tl_text) and edit operations (I = Insertion, D = Deletion, S = Substitution) performed on the field. Replace \\\\n with \\n to show the three aligned rows.\n",
    "\n",
    "\n",
    "After cleaning the data, we convert the categirial labels to onhot codes with OneHot Eecoder for both Subject and Modality. Then, along with the existing features, we formed some **new features** on input texts(src_text,mt_text and tgt_text) as follows:\n",
    "\n",
    ">**Number of words** in source,target and mt texts seperately,\n",
    "\n",
    ">**Number of unique words** in source,target and mt texts seperately,\n",
    "\n",
    ">**Number of unique words- translator wise** in source,target and mt texts seperately aggregated by translators,\n",
    "\n",
    ">**Number of unique words- modality wise** in source,target and mt texts seperately aggregated by modalities,\n",
    "\n",
    ">**Time of each word** in the entire edit time,\n",
    "\n",
    ">**Ratio of number of 300ms and 1000ms pauses** during the edit time,\n",
    "\n",
    ">**Number of Insert(I), Number of Delete(D) and Sunstitute(S) operations plus sum of them as IDS** features based on the Aligned_text, \n",
    "\n",
    ">**Linguistic features** extracted from source text and target text (number of Noun,Verb,Det,Adj,Adp,Adv,Pron,PropN,Aux,Sym,Punct,Part),\n",
    "\n",
    ">**Number the word with length less then 3 ,4** as **AbnormalWords** which are subtracted by DETerminers(determiners,articles) and PUCTuations and Adpositions, \n",
    "\n",
    ">**Abnormal tokens ratio** for source and target texts based on the total number of tokens.\n",
    "\n",
    ">**Differences of linguistic features** from source abd target text\n",
    " \n",
    "\n",
    "#### Test Sets    \n",
    "The three test splits (one per configuration) contain the same 120 entries each, following the same structure as train. Each test split omit some of the fields to prevent leakage of information.\n",
    "\n",
    "Based on the above preprocessing function that is specialized to train set, the other preprocessing functions provided for test sets are as follows. \n",
    "\n",
    ">**Preprocessing_MaskSubject()**: this function adds/removes some features based on the **test_mask_subject** test data in which the **subject_id** is absent, for the main task of post-editor stylometry. \n",
    "\n",
    ">**Preprocessing_MaskModality()**: this function adds/removes some features based on the **test_mask_modality** test data in which the following fields are absent for the modality prediction extra task: **modality, mt_text, n_insert, n_delete, n_substitute, n_shift, ter, bleu, chrf, aligned_edit**.\n",
    "\n",
    ">**Preprocessing_MaskTime()**: this function adds/removes some features based on the **test_mask_time** test data in which the following fields are absent for the time and pause prediction extra task: **edit_time, n_pause_geq_300, len_pause_geq_300, n_pause_geq_1000, and len_pause_geq_1000**.\n",
    "\n",
    "<font color='red'>Note: for each preprocessing function, a corresponding createDataFrame function is provided to collect the data from preprocessing and create the final data frame that is ready to be injected into the ML model.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining constant variables\n",
    "class constants:\n",
    "    Param_n_pause_geq_300='n_pause_geq_300'\n",
    "    Param_len_pause_geq_300='len_pause_geq_300'\n",
    "    Param_n_pause_geq_1000='n_pause_geq_1000'\n",
    "    Param_len_pause_geq_1000='len_pause_geq_1000'\n",
    "    Param_edit_time='edit_time'\n",
    "    Param_n300_ratio= 'n300_ratio'\n",
    "    Param_n1000_ratio='n1000_ratio'\n",
    "    Param_TER = 'ter'\n",
    "    Param_BLEU='bleu'\n",
    "    Param_CHRF='chrf'\n",
    "    Param_num_word_src='num_word_src'\n",
    "    Param_num_word_mt='num_word_mt'\n",
    "    Param_num_word_tgt='num_word_tgt'\n",
    "    Param_num_word_unique_mt='num_word_unique_mt'\n",
    "    Param_num_word_unique_tgt='num_word_unique_tgt'\n",
    "    Param_num_unique_mt_t_wise='num_unique_mt_t_wise'\n",
    "    Param_num_unique_mt_m_wise='num_unique_mt_m_wise'\n",
    "    Param_num_unique_tgt_t_wise='num_unique_tgt_t_wise'\n",
    "    Param_num_unique_tgt_m_wise='num_unique_tgt_m_wise'\n",
    "    subjects=['t1','t2','t3']\n",
    "    modalities=['ht','pe1','pe2']\n",
    "    s_mt_t_wise=[]\n",
    "    s_tgt_t_wise=[]\n",
    "    s_mt_m_wise=[]\n",
    "    s_tgt_m_wise=[]\n",
    "    Param_item_id='item_id'\n",
    "    Param_subject_id='subject_id'\n",
    "    Param_modality='modality'\n",
    "    Param_src_text='src_text'\n",
    "    Param_mt_text='mt_text'\n",
    "    Param_tgt_text='tgt_text'\n",
    "    Param_edit_time='edit_time'\n",
    "    Param_k_total='k_total'\n",
    "    Param_k_letter='k_letter'\n",
    "    Param_k_digit='k_digit'\n",
    "    Param_k_white='k_white'\n",
    "    Param_k_symbol='k_symbol'\n",
    "    Param_k_nav='k_nav'\n",
    "    Param_k_erase='k_erase'\n",
    "    Param_k_copy='k_copy'\n",
    "    Param_k_cut='k_cut'\n",
    "    Param_k_paste='k_paste'\n",
    "    Param_num_annotations='num_annotations'\n",
    "    Param_n_insert='n_insert'\n",
    "    Param_n_delete='n_delete'\n",
    "    Param_n_substitute='n_substitute'\n",
    "    Param_n_shift='n_shift'\n",
    "    Param_aligned_edit='aligned_edit'\n",
    "    Param_seperated_aligned_edit='seperated_aligned_edit'\n",
    "    Param_num_of_I='num_of_I'\n",
    "    Param_num_of_D='num_of_D'\n",
    "    Param_num_of_S='num_of_S'\n",
    "    Param_num_of_IDS='num_of_IDS'\n",
    "    Param_translator1='t1'\n",
    "    Param_translator2='t2'\n",
    "    Param_translator3='t3'\n",
    "    Param_modality_ht='ht'\n",
    "    Param_modality_pe1='pe1'\n",
    "    Param_modality_pe2='pe2'\n",
    "    Param_num_words_per_edit_time='word_time'\n",
    "    #tgt linguistic featurs name\n",
    "    Param_tgt_countWord4='tgt_countWord4'\n",
    "    Param_tgt_countWord3='tgt_countWord3'\n",
    "    Param_tgt_totalNumberOfTokens='tgt_totalNumberOfTokens'\n",
    "    Param_tgt_ratioAbnormal='tgt_ratioAbnormal'\n",
    "    Param_tgt_countAbnormlWord='tgt_countAbnormlWord'\n",
    "    Param_tgt_NOUN='tgt_NOUN'\n",
    "    Param_tgt_VERB='tgt_VERB'\n",
    "    Param_tgt_ADJ='tgt_ADJ'\n",
    "    Param_tgt_ADV='tgt_ADV'\n",
    "    Param_tgt_PROPN='tgt_PROPN'\n",
    "    Param_tgt_DET='tgt_DET'\n",
    "    Param_tgt_PRT='tgt_PRT'\n",
    "    Param_tgt_CONJ='tgt_CONJ'\n",
    "    Param_tgt_PUNCT='tgt_PUNCT'\n",
    "    Param_tgt_ADP='tgt_ADP'\n",
    "    Param_tgt_AUX='tgt_AUX'\n",
    "    Param_tgt_CCONJ='tgt_CCONJ'\n",
    "    Param_tgt_INTJ='tgt_INTJ'\n",
    "    Param_tgt_NUM='tgt_NUM'\n",
    "    Param_tgt_PART='tgt_PART'\n",
    "    Param_tgt_PRON='tgt_PRON'\n",
    "    Param_tgt_SCONJ='tgt_SCONJ'\n",
    "    Param_tgt_SYM='tgt_SYM'\n",
    "\n",
    "    #src linguistic featurs name\n",
    "    Param_src_countWord4='src_countWord4'\n",
    "    Param_src_countWord3='src_countWord3'\n",
    "    Param_src_totalNumberOfTokens='src_totalNumberOfTokens'\n",
    "    Param_src_ratioAbnormal='src_ratioAbnormal'\n",
    "    Param_src_countAbnormlWord='src_countAbnormlWord'\n",
    "    Param_src_NOUN='src_NOUN'\n",
    "    Param_src_VERB='src_VERB'\n",
    "    Param_src_ADJ='src_ADJ'\n",
    "    Param_src_ADV='src_ADV'\n",
    "    Param_src_PROPN='src_PROPN'\n",
    "    Param_src_DET='src_DET'\n",
    "    Param_src_PRT='src_PRT'\n",
    "    Param_src_CONJ='src_CONJ'\n",
    "    Param_src_PUNCT='src_PUNCT'\n",
    "    Param_src_ADP='src_ADP'\n",
    "    Param_src_AUX='src_AUX'\n",
    "    Param_src_CCONJ='src_CCONJ'\n",
    "    Param_src_INTJ='src_INTJ'\n",
    "    Param_src_NUM='src_NUM'\n",
    "    Param_src_PART='src_PART'\n",
    "    Param_src_PRON='src_PRON'\n",
    "    Param_src_SCONJ='src_SCONJ'\n",
    "    Param_src_SYM='src_SYM'\n",
    "\n",
    "    #linguistic featurs diff\n",
    "    Param_diff_countWord4='diff_countWord4'\n",
    "    Param_diff_countWord3='diff_countWord3'\n",
    "    Param_diff_totalNumberOfTokens='diff_totalNumberOfTokens'\n",
    "    Param_diff_ratioAbnormal='diff_ratioAbnormal'\n",
    "    Param_diff_countAbnormlWord='diff_countAbnormlWord'\n",
    "    Param_diff_NOUN='diff_NOUN'\n",
    "    Param_diff_VERB='diff_VERB'\n",
    "    Param_diff_ADJ='diff_ADJ'\n",
    "    Param_diff_ADV='diff_ADV'\n",
    "    Param_diff_PROPN='diff_PROPN'\n",
    "    Param_diff_DET='diff_DET'\n",
    "    Param_diff_PRT='diff_PRT'\n",
    "    Param_diff_CONJ='diff_CONJ'\n",
    "    Param_diff_PUNCT='diff_PUNCT'\n",
    "    Param_diff_ADP='diff_ADP'\n",
    "    Param_diff_AUX='diff_AUX'\n",
    "    Param_diff_CCONJ='diff_CCONJ'\n",
    "    Param_diff_INTJ='diff_INTJ'\n",
    "    Param_diff_NUM='diff_NUM'\n",
    "    Param_diff_PART='diff_PART'\n",
    "    Param_diff_PRON='diff_PRON'\n",
    "    Param_diff_SCONJ='diff_SCONJ'\n",
    "    Param_diff_SYM='diff_SYM'\n",
    "    \n",
    "    \n",
    "# Doing preprocessing for the  train dataset\n",
    "def PreProcess(df):\n",
    "    \n",
    "    print('Starting preprocessing ....')\n",
    "    if df.isnull().sum().all().sum()>0:\n",
    "        print('\\n Data has some null values')\n",
    "        print('\\n Fill NaN value to 0')\n",
    "        df=df.fillna(0)\n",
    "        print('\\n done')\n",
    "        \n",
    "    print('\\n Create dummies with OneHot encoder for subjects and modalities')\n",
    "    translator='subject_id'\n",
    "    modality='modality'\n",
    "    \n",
    "    traslator_vectors = {}\n",
    "    modality_vectors = {}\n",
    "    i=1\n",
    "    modality_vectors = pd.get_dummies(df[modality])\n",
    "    traslator_vectors = pd.get_dummies(df[translator])\n",
    "\n",
    "    for subject in constants.subjects:\n",
    "        df[subject]=traslator_vectors[subject]\n",
    "\n",
    "    for modality in constants.modalities:\n",
    "        df[modality]=modality_vectors[modality]\n",
    "\n",
    "    print('done \\n Replace nan string with empty in aligned_edit and mt_text')\n",
    "    \n",
    "    #replace nan string in aligned_edit and mt_text\n",
    "    df[constants.Param_aligned_edit].replace('nan','',inplace=True)\n",
    "    df[constants.Param_mt_text].replace('nan','',inplace=True)\n",
    "    \n",
    "    print('done \\n Creating new columns number of words and number of uniaue words for src_text, mt_text and tgt_text')\n",
    "    # Number of words in src_text,mt_text,tgt_text\n",
    "    df[constants.Param_num_word_src] = df['src_text'].apply(lambda x: len(str(x).split()))\n",
    "    df[constants.Param_num_word_mt] = df[constants.Param_mt_text].apply(lambda x: len(str(x).split()))\n",
    "    df[constants.Param_num_word_tgt] = df[constants.Param_tgt_text].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "    # time to each word in the entire edit time\n",
    "    df[constants.Param_num_words_per_edit_time]=df[constants.Param_edit_time]/df[constants.Param_num_word_tgt]\n",
    "\n",
    "    # Number of unigue words in src_text,mt_text,tgt_text\n",
    "    #df[\"num_unique_src\"] = df[\"src_text\"].apply(lambda x: len(set(str(x).split())))\n",
    "    df[constants.Param_num_word_unique_mt] = df[constants.Param_mt_text].apply(lambda x: len(set(str(x).split())))\n",
    "    df[constants.Param_num_word_unique_tgt] = df[constants.Param_tgt_text].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "    \n",
    "    # Number of unigue words in mt_text group by subject \n",
    "    constants.s_mt_t_wise=df.groupby(constants.Param_subject_id)[constants.Param_mt_text].agg(lambda x: ' '.join(set(x)))\n",
    "    constants.s_mt_t_wise = constants.s_mt_t_wise.apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "    # Number of unigue words in tgt_text group by subject \n",
    "    constants.s_tgt_t_wise=df.groupby(constants.Param_subject_id)[constants.Param_tgt_text].agg(lambda x: ' '.join(set(x)))\n",
    "    constants.s_tgt_t_wise = constants.s_tgt_t_wise.apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "    # Number of unigue words in mt_text group by modality \n",
    "    constants.s_mt_m_wise=df.groupby(constants.Param_modality)[constants.Param_mt_text].agg(lambda x: ' '.join(set(x)))\n",
    "    constants.s_mt_m_wise = constants.s_mt_m_wise.apply(lambda x: len(set(str(x).split())))\n",
    "    \n",
    "    # Number of unigue words in tgt_text group by modality\n",
    "    constants.s_tgt_m_wise=df.groupby(constants.Param_modality)[constants.Param_tgt_text].agg(lambda x: ' '.join(set(x)))\n",
    "    constants.s_tgt_m_wise= constants.s_tgt_m_wise.apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "    # Calculating Ratio 300 & 1000\n",
    "    df[constants.Param_n300_ratio]=(df[constants.Param_n_pause_geq_300]/df[constants.Param_edit_time])*100\n",
    "    df[constants.Param_n1000_ratio]=(df[constants.Param_n_pause_geq_1000]/df[constants.Param_edit_time])*100\n",
    "\n",
    "    # Setting unique numbers(above values) in the dataframe\n",
    "    for subject in constants.subjects:\n",
    "        df.loc[(df[constants.Param_subject_id] == subject), 'num_unique_mt_t_wise']=constants.s_mt_t_wise[subject]\n",
    "        df.loc[(df[constants.Param_subject_id] == subject), 'num_unique_tgt_t_wise']=constants.s_tgt_t_wise[subject]\n",
    "\n",
    "    for modality in constants.modalities:\n",
    "        df.loc[(df[constants.Param_modality] == modality), 'num_unique_mt_m_wise']=constants.s_mt_m_wise[modality]\n",
    "        df.loc[(df[constants.Param_modality] == modality), 'num_unique_tgt_m_wise']=constants.s_tgt_m_wise[modality]\n",
    "    \n",
    "    # Aligned text extraction\n",
    "    new= df[constants.Param_aligned_edit].str.split(\"EVAL:\", n = 1, expand = True)\n",
    "    df[constants.Param_seperated_aligned_edit] =new[1]\n",
    "    df[constants.Param_seperated_aligned_edit].replace('NONE','',inplace=True)\n",
    "    df[constants.Param_num_of_I] = df[constants.Param_seperated_aligned_edit].apply(lambda x: len(str(x).split('I'))-1)\n",
    "    df[constants.Param_num_of_D] = df[constants.Param_seperated_aligned_edit].apply(lambda x: len(str(x).split('D'))-1)\n",
    "    df[constants.Param_num_of_S] = df[constants.Param_seperated_aligned_edit].apply(lambda x: len(str(x).split('S'))-1)\n",
    "    df[constants.Param_num_of_IDS] = df[constants.Param_num_of_I]+df[constants.Param_num_of_D]+df[constants.Param_num_of_S]\n",
    "\n",
    "    \n",
    "    df = df.drop(constants.Param_seperated_aligned_edit, 1)\n",
    "\n",
    "    # linguistic extraction from src_text  + ratio Abnormal tokens\n",
    "    text=constants.Param_src_text\n",
    "    #getting the word with length less then 3 ,4 -> we will compare them to DETerminrs(determiners and articles) and PUCTuations later \n",
    "    # to clean it and reach to the abnormal word which machine translators might produce\n",
    "    df[constants.Param_src_countWord3]=df[text].map( lambda x: countWords_Src(x,3))\n",
    "    df[constants.Param_src_countWord4]=df[text].map( lambda x: countWords_Src(x,4))\n",
    "    \n",
    "    df[constants.Param_src_totalNumberOfTokens]=df[text].map( lambda x: getNumberOfTokens_Src(x))\n",
    "    df[constants.Param_src_NOUN]=df[text].map( lambda x: getCount_Src(x,[NOUN]))\n",
    "    df[constants.Param_src_VERB]=df[text].map( lambda x: getCount_Src(x,[VERB]))\n",
    "    df[constants.Param_src_ADJ]=df[text].map( lambda x: getCount_Src(x,[ADJ]))\n",
    "    df[constants.Param_src_ADV]=df[text].map( lambda x: getCount_Src(x,[ADV]))\n",
    "    df[constants.Param_src_PROPN]=df[text].map( lambda x: getCount_Src(x,[PROPN]))\n",
    "    df[constants.Param_src_PRON]=df[text].map( lambda x: getCount_Src(x,[PRON]))\n",
    "    df[constants.Param_src_AUX]=df[text].map( lambda x: getCount_Src(x,[AUX]))\n",
    "    df[constants.Param_src_CONJ]=df[text].map( lambda x: getCount_Src(x,[CONJ,CCONJ,SCONJ]))\n",
    "    df[constants.Param_src_ADP]=df[text].map( lambda x: getCount_Src(x,[ADP]))\n",
    "    df[constants.Param_src_DET]=df[text].map( lambda x: getCount_Src(x,[DET]))\n",
    "    df[constants.Param_src_SYM]=df[text].map( lambda x: getCount_Src(x,[SYM]))\n",
    "    df[constants.Param_src_PART]=df[text].map( lambda x: getCount_Src(x,[PART]))\n",
    "    df[constants.Param_src_PUNCT]=df[text].map( lambda x: getCount_Src(x,[PUNCT]))\n",
    "    df[constants.Param_src_countAbnormlWord]=(df[constants.Param_src_countWord4]+df[constants.Param_src_countWord3])-(df[constants.Param_src_DET]+df[constants.Param_src_PUNCT]+df[constants.Param_src_ADP])\n",
    "    \n",
    "    df[constants.Param_src_ratioAbnormal]=(df[constants.Param_src_countAbnormlWord]/df[constants.Param_src_totalNumberOfTokens])*100\n",
    "    df[constants.Param_src_ratioAbnormal]=df[constants.Param_src_ratioAbnormal].replace(np.Infinity,0)\n",
    "   \n",
    "                                          \n",
    "\n",
    "\n",
    "    # linguistic tgt_text features + ratio Abnormal tokens\n",
    "    text=constants.Param_tgt_text\n",
    "    df[constants.Param_tgt_countWord3]=df[text].map( lambda x: countWords(x,3))\n",
    "    df[constants.Param_tgt_countWord4]=df[text].map( lambda x: countWords(x,4))\n",
    "    df[constants.Param_tgt_totalNumberOfTokens]=df[text].map( lambda x: getNumberOfTokens(x))\n",
    "    df[constants.Param_tgt_NOUN]=df[text].map( lambda x: getCount(x,[NOUN]))\n",
    "    df[constants.Param_tgt_VERB]=df[text].map( lambda x: getCount(x,[VERB]))\n",
    "    df[constants.Param_tgt_ADJ]=df[text].map( lambda x: getCount(x,[ADJ]))\n",
    "    df[constants.Param_tgt_ADV]=df[text].map( lambda x: getCount(x,[ADV]))\n",
    "    df[constants.Param_tgt_PROPN]=df[text].map( lambda x: getCount(x,[PROPN]))\n",
    "    df[constants.Param_tgt_PRON]=df[text].map( lambda x: getCount(x,[PRON]))\n",
    "    df[constants.Param_tgt_AUX]=df[text].map( lambda x: getCount(x,[AUX]))\n",
    "    df[constants.Param_tgt_CONJ]=df[text].map( lambda x: getCount(x,[CONJ,CCONJ,SCONJ]))\n",
    "    df[constants.Param_tgt_ADP]=df[text].map( lambda x: getCount(x,[ADP]))\n",
    "    df[constants.Param_tgt_DET]=df[text].map( lambda x: getCount(x,[DET]))\n",
    "    df[constants.Param_tgt_SYM]=df[text].map( lambda x: getCount(x,[SYM]))\n",
    "    df[constants.Param_tgt_PART]=df[text].map( lambda x: getCount(x,[PART]))\n",
    "    df[constants.Param_tgt_PUNCT]=df[text].map( lambda x: getCount(x,[PUNCT]))\n",
    "    df[constants.Param_tgt_countAbnormlWord]=(df[constants.Param_tgt_countWord4]+df[constants.Param_tgt_countWord3])-(df[constants.Param_tgt_DET]+df[constants.Param_tgt_PUNCT]+df[constants.Param_tgt_ADP])\n",
    "    df[constants.Param_tgt_ratioAbnormal]=(df[constants.Param_tgt_countAbnormlWord]/df[constants.Param_tgt_totalNumberOfTokens])*100\n",
    "    df[constants.Param_tgt_ratioAbnormal]=df[constants.Param_tgt_ratioAbnormal]\n",
    "    # linguistic featurs diff \n",
    "    df[constants.Param_diff_countWord3]=((df[constants.Param_tgt_countWord3]-df[constants.Param_src_countWord3])/df[constants.Param_tgt_countWord3])\n",
    "    df[constants.Param_diff_countWord4]=((df[constants.Param_tgt_countWord4]-df[constants.Param_src_countWord4])/df[constants.Param_tgt_countWord4])\n",
    "    df[constants.Param_diff_totalNumberOfTokens]=((df[constants.Param_tgt_totalNumberOfTokens]-df[constants.Param_src_totalNumberOfTokens])/df[constants.Param_tgt_totalNumberOfTokens])\n",
    "    df[constants.Param_diff_NOUN]=((df[constants.Param_tgt_NOUN]-df[constants.Param_src_NOUN])/df[constants.Param_tgt_NOUN])\n",
    "    df[constants.Param_diff_VERB]=((df[constants.Param_tgt_VERB]-df[constants.Param_src_VERB])/df[constants.Param_tgt_VERB])\n",
    "    df[constants.Param_diff_ADJ]=((df[constants.Param_tgt_ADJ]-df[constants.Param_src_ADJ])/df[constants.Param_tgt_ADJ])\n",
    "    df[constants.Param_diff_ADV]=((df[constants.Param_tgt_ADV]-df[constants.Param_src_ADV])/df[constants.Param_tgt_ADV])\n",
    "    df[constants.Param_diff_PROPN]=((df[constants.Param_tgt_PROPN]-df[constants.Param_src_PROPN])/df[constants.Param_tgt_PROPN])\n",
    "    df[constants.Param_diff_PRON]=((df[constants.Param_tgt_PRON]-df[constants.Param_src_PRON])/df[constants.Param_tgt_PRON])\n",
    "    df[constants.Param_diff_AUX]=((df[constants.Param_tgt_AUX]-df[constants.Param_src_AUX])/df[constants.Param_tgt_AUX])\n",
    "    df[constants.Param_diff_CONJ]=((df[constants.Param_tgt_CONJ]-df[constants.Param_src_CONJ])/df[constants.Param_tgt_CONJ])\n",
    "    df[constants.Param_diff_ADP]=((df[constants.Param_tgt_ADP]-df[constants.Param_src_ADP])/df[constants.Param_tgt_ADP])\n",
    "    df[constants.Param_diff_DET]=((df[constants.Param_tgt_DET]-df[constants.Param_src_DET])/df[constants.Param_tgt_DET])\n",
    "    df[constants.Param_diff_SYM]=((df[constants.Param_tgt_SYM]-df[constants.Param_src_SYM])/df[constants.Param_tgt_SYM])\n",
    "    df[constants.Param_diff_PART]=((df[constants.Param_tgt_PART]-df[constants.Param_src_PART])/df[constants.Param_tgt_PART]).replace(np.NINF,0)\n",
    "    df[constants.Param_diff_PUNCT]=((df[constants.Param_tgt_PUNCT]-df[constants.Param_src_PUNCT])/df[constants.Param_tgt_PUNCT])\n",
    "    df[constants.Param_diff_countAbnormlWord]=(( df[constants.Param_tgt_countAbnormlWord]- df[constants.Param_src_countAbnormlWord])/ df[constants.Param_tgt_countAbnormlWord]).replace(np.Infinity,0)\n",
    "    df[constants.Param_diff_ratioAbnormal]=((df[constants.Param_tgt_ratioAbnormal]-df[constants.Param_src_ratioAbnormal])/df[constants.Param_tgt_ratioAbnormal])\n",
    "    df=df.fillna(0)\n",
    "    df=df.replace(np.Infinity,0)\n",
    "    return df\n",
    "   \n",
    "def PreProcess_MaskSubject(df):\n",
    "    \n",
    "    print('Starting preprocessing ....')\n",
    "    if df.isnull().sum().all().sum()>0:\n",
    "        print('\\n Data has some null values')\n",
    "        print('\\n Fill NaN value to 0')\n",
    "        df=df.fillna(0)\n",
    "        print('\\n done')\n",
    "        \n",
    "    print('\\n Create dummies with OneHot encoder for subjects and modalities')\n",
    "    #translator='subject_id'\n",
    "    modality='modality'\n",
    "    \n",
    "    #traslator_vectors = {}\n",
    "    modality_vectors = {}\n",
    "\n",
    "    #traslator_vectors = pd.get_dummies(df[translator])\n",
    "    modality_vectors = pd.get_dummies(df[modality])\n",
    "    i=1\n",
    "    #for subject in constants.subjects:\n",
    "        #df[subject]=traslator_vectors[subject]\n",
    "\n",
    "    for modality in constants.modalities:\n",
    "        df[modality]=modality_vectors[modality]\n",
    "\n",
    "    print('done \\n Replace nan string with empty in aligned_edit and mt_text')\n",
    "    #replace nan string in aligned_edit and mt_text\n",
    "    df[constants.Param_aligned_edit].replace('nan','',inplace=True)\n",
    "    df[constants.Param_mt_text].replace('nan','',inplace=True)\n",
    "    \n",
    "    print('done \\n Creating new columns number of words and number of uniaue words for src_text, mt_text and tgt_text')\n",
    "    # Number of words in src_text,mt_text,tgt_text\n",
    "    df[constants.Param_num_word_src] = df['src_text'].apply(lambda x: len(str(x).split()))\n",
    "    df[constants.Param_num_word_mt] = df[constants.Param_mt_text].apply(lambda x: len(str(x).split()))\n",
    "    df[constants.Param_num_word_tgt] = df[constants.Param_tgt_text].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "    # Number of unigue words in src_text,mt_text,tgt_text\n",
    "    #df[\"num_unique_src\"] = df[\"src_text\"].apply(lambda x: len(set(str(x).split())))\n",
    "    df[constants.Param_num_word_unique_mt] = df[constants.Param_mt_text].apply(lambda x: len(set(str(x).split())))\n",
    "    df[constants.Param_num_word_unique_tgt] = df[constants.Param_tgt_text].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "     # time to each word in the entire edit time\n",
    "    df[constants.Param_num_words_per_edit_time]=df[constants.Param_edit_time]/df[constants.Param_num_word_tgt]\n",
    "    \n",
    "    # Number of unigue words in mt_text group by subject \n",
    "    #constants.s_mt_t_wise=df.groupby(constants.Param_subject_id)[constants.Param_mt_text].agg(lambda x: ' '.join(set(x)))\n",
    "    #constants.s_mt_t_wise = constants.s_mt_t_wise.apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "    # Number of unigue words in tgt_text group by subject \n",
    "    #constants.s_tgt_t_wise=df.groupby(constants.Param_subject_id)[constants.Param_tgt_text].agg(lambda x: ' '.join(set(x)))\n",
    "    #constants.s_tgt_t_wise = constants.s_tgt_t_wise.apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "    # Number of unigue words in mt_text group by modality \n",
    "    constants.s_mt_m_wise=df.groupby(constants.Param_modality)[constants.Param_mt_text].agg(lambda x: ' '.join(set(x)))\n",
    "    constants.s_mt_m_wise = constants.s_mt_m_wise.apply(lambda x: len(set(str(x).split())))\n",
    "    \n",
    "    # Number of unigue words in tgt_text group by modality\n",
    "    constants.s_tgt_m_wise=df.groupby(constants.Param_modality)[constants.Param_tgt_text].agg(lambda x: ' '.join(set(x)))\n",
    "    constants.s_tgt_m_wise= constants.s_tgt_m_wise.apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "    # Calculating Ratio 300 & 1000\n",
    "    df[constants.Param_n300_ratio]=(df[constants.Param_n_pause_geq_300]/df[constants.Param_edit_time])*100\n",
    "    df[constants.Param_n1000_ratio]=(df[constants.Param_n_pause_geq_1000]/df[constants.Param_edit_time])*100\n",
    "\n",
    "    # Setting unique numbers(above values) in the dataframe\n",
    "    #for subject in constants.subjects:\n",
    "        #df.loc[(df[constants.Param_subject_id] == subject), 'num_unique_mt_t_wise']=constants.s_mt_t_wise[subject]\n",
    "        #df.loc[(df[constants.Param_subject_id] == subject), 'num_unique_tgt_t_wise']=constants.s_tgt_t_wise[subject]\n",
    "\n",
    "    for modality in constants.modalities:\n",
    "        df.loc[(df[constants.Param_modality] == modality), 'num_unique_mt_m_wise']=constants.s_mt_m_wise[modality]\n",
    "        df.loc[(df[constants.Param_modality] == modality), 'num_unique_tgt_m_wise']=constants.s_tgt_m_wise[modality]\n",
    "    \n",
    "    # Aligned text extraction\n",
    "    new= df[constants.Param_aligned_edit].str.split(\"EVAL:\", n = 1, expand = True)\n",
    "    df[constants.Param_seperated_aligned_edit] =new[1]\n",
    "    df[constants.Param_seperated_aligned_edit].replace('NONE','',inplace=True)\n",
    "    df[constants.Param_num_of_I] = df[constants.Param_seperated_aligned_edit].apply(lambda x: len(str(x).split('I'))-1)\n",
    "    df[constants.Param_num_of_D] = df[constants.Param_seperated_aligned_edit].apply(lambda x: len(str(x).split('D'))-1)\n",
    "    df[constants.Param_num_of_S] = df[constants.Param_seperated_aligned_edit].apply(lambda x: len(str(x).split('S'))-1)\n",
    "    df[constants.Param_num_of_IDS] = df[constants.Param_num_of_I]+df[constants.Param_num_of_D]+df[constants.Param_num_of_S]\n",
    "    df = df.drop(constants.Param_seperated_aligned_edit, 1)\n",
    "\n",
    "    # linguistic src_text features + ratio Abnormal tokens\n",
    "    text=constants.Param_src_text\n",
    "    df[constants.Param_src_countWord3]=df[text].map( lambda x: countWords_Src(x,3))\n",
    "    df[constants.Param_src_countWord4]=df[text].map( lambda x: countWords_Src(x,4))\n",
    "    df[constants.Param_src_totalNumberOfTokens]=df[text].map( lambda x: getNumberOfTokens_Src(x))\n",
    "    df[constants.Param_src_NOUN]=df[text].map( lambda x: getCount_Src(x,[NOUN]))\n",
    "    df[constants.Param_src_VERB]=df[text].map( lambda x: getCount_Src(x,[VERB]))\n",
    "    df[constants.Param_src_ADJ]=df[text].map( lambda x: getCount_Src(x,[ADJ]))\n",
    "    df[constants.Param_src_ADV]=df[text].map( lambda x: getCount_Src(x,[ADV]))\n",
    "    df[constants.Param_src_PROPN]=df[text].map( lambda x: getCount_Src(x,[PROPN]))\n",
    "    df[constants.Param_src_PRON]=df[text].map( lambda x: getCount_Src(x,[PRON]))\n",
    "    df[constants.Param_src_AUX]=df[text].map( lambda x: getCount_Src(x,[AUX]))\n",
    "    df[constants.Param_src_CONJ]=df[text].map( lambda x: getCount_Src(x,[CONJ,CCONJ,SCONJ]))\n",
    "    df[constants.Param_src_ADP]=df[text].map( lambda x: getCount_Src(x,[ADP]))\n",
    "    df[constants.Param_src_DET]=df[text].map( lambda x: getCount_Src(x,[DET]))\n",
    "    df[constants.Param_src_SYM]=df[text].map( lambda x: getCount_Src(x,[SYM]))\n",
    "    df[constants.Param_src_PART]=df[text].map( lambda x: getCount_Src(x,[PART]))\n",
    "    df[constants.Param_src_PUNCT]=df[text].map( lambda x: getCount_Src(x,[PUNCT]))\n",
    "    df[constants.Param_src_countAbnormlWord]=(df[constants.Param_src_countWord4]+df[constants.Param_src_countWord3])-(df[constants.Param_src_DET]+df[constants.Param_src_PUNCT]+df[constants.Param_src_ADP])\n",
    "    \n",
    "    df[constants.Param_src_ratioAbnormal]=(df[constants.Param_src_countAbnormlWord]/df[constants.Param_src_totalNumberOfTokens])*100\n",
    "    df[constants.Param_src_ratioAbnormal]=df[constants.Param_src_ratioAbnormal].replace(np.Infinity,0)\n",
    "   \n",
    "                                          \n",
    "\n",
    "\n",
    "    # linguistic tgt_text features + ratio Abnormal tokens\n",
    "    text=constants.Param_tgt_text\n",
    "    df[constants.Param_tgt_countWord3]=df[text].map( lambda x: countWords(x,3))\n",
    "    df[constants.Param_tgt_countWord4]=df[text].map( lambda x: countWords(x,4))\n",
    "    df[constants.Param_tgt_totalNumberOfTokens]=df[text].map( lambda x: getNumberOfTokens(x))\n",
    "    df[constants.Param_tgt_NOUN]=df[text].map( lambda x: getCount(x,[NOUN]))\n",
    "    df[constants.Param_tgt_VERB]=df[text].map( lambda x: getCount(x,[VERB]))\n",
    "    df[constants.Param_tgt_ADJ]=df[text].map( lambda x: getCount(x,[ADJ]))\n",
    "    df[constants.Param_tgt_ADV]=df[text].map( lambda x: getCount(x,[ADV]))\n",
    "    df[constants.Param_tgt_PROPN]=df[text].map( lambda x: getCount(x,[PROPN]))\n",
    "    df[constants.Param_tgt_PRON]=df[text].map( lambda x: getCount(x,[PRON]))\n",
    "    df[constants.Param_tgt_AUX]=df[text].map( lambda x: getCount(x,[AUX]))\n",
    "    df[constants.Param_tgt_CONJ]=df[text].map( lambda x: getCount(x,[CONJ,CCONJ,SCONJ]))\n",
    "    df[constants.Param_tgt_ADP]=df[text].map( lambda x: getCount(x,[ADP]))\n",
    "    df[constants.Param_tgt_DET]=df[text].map( lambda x: getCount(x,[DET]))\n",
    "    df[constants.Param_tgt_SYM]=df[text].map( lambda x: getCount(x,[SYM]))\n",
    "    df[constants.Param_tgt_PART]=df[text].map( lambda x: getCount(x,[PART]))\n",
    "    df[constants.Param_tgt_PUNCT]=df[text].map( lambda x: getCount(x,[PUNCT]))\n",
    "    df[constants.Param_tgt_countAbnormlWord]=(df[constants.Param_tgt_countWord4]+df[constants.Param_tgt_countWord3])-(df[constants.Param_tgt_DET]+df[constants.Param_tgt_PUNCT]+df[constants.Param_tgt_ADP])\n",
    "    df[constants.Param_tgt_ratioAbnormal]=(df[constants.Param_tgt_countAbnormlWord]/df[constants.Param_tgt_totalNumberOfTokens])*100\n",
    "    df[constants.Param_tgt_ratioAbnormal]=df[constants.Param_tgt_ratioAbnormal]\n",
    "    # linguistic featurs diff \n",
    "    df[constants.Param_diff_countWord3]=((df[constants.Param_tgt_countWord3]-df[constants.Param_src_countWord3])/df[constants.Param_tgt_countWord3])\n",
    "    df[constants.Param_diff_countWord4]=((df[constants.Param_tgt_countWord4]-df[constants.Param_src_countWord4])/df[constants.Param_tgt_countWord4])\n",
    "    df[constants.Param_diff_totalNumberOfTokens]=((df[constants.Param_tgt_totalNumberOfTokens]-df[constants.Param_src_totalNumberOfTokens])/df[constants.Param_tgt_totalNumberOfTokens])\n",
    "    df[constants.Param_diff_NOUN]=((df[constants.Param_tgt_NOUN]-df[constants.Param_src_NOUN])/df[constants.Param_tgt_NOUN])\n",
    "    df[constants.Param_diff_VERB]=((df[constants.Param_tgt_VERB]-df[constants.Param_src_VERB])/df[constants.Param_tgt_VERB])\n",
    "    df[constants.Param_diff_ADJ]=((df[constants.Param_tgt_ADJ]-df[constants.Param_src_ADJ])/df[constants.Param_tgt_ADJ])\n",
    "    df[constants.Param_diff_ADV]=((df[constants.Param_tgt_ADV]-df[constants.Param_src_ADV])/df[constants.Param_tgt_ADV])\n",
    "    df[constants.Param_diff_PROPN]=((df[constants.Param_tgt_PROPN]-df[constants.Param_src_PROPN])/df[constants.Param_tgt_PROPN])\n",
    "    df[constants.Param_diff_PRON]=((df[constants.Param_tgt_PRON]-df[constants.Param_src_PRON])/df[constants.Param_tgt_PRON])\n",
    "    df[constants.Param_diff_AUX]=((df[constants.Param_tgt_AUX]-df[constants.Param_src_AUX])/df[constants.Param_tgt_AUX])\n",
    "    df[constants.Param_diff_CONJ]=((df[constants.Param_tgt_CONJ]-df[constants.Param_src_CONJ])/df[constants.Param_tgt_CONJ])\n",
    "    df[constants.Param_diff_ADP]=((df[constants.Param_tgt_ADP]-df[constants.Param_src_ADP])/df[constants.Param_tgt_ADP])\n",
    "    df[constants.Param_diff_DET]=((df[constants.Param_tgt_DET]-df[constants.Param_src_DET])/df[constants.Param_tgt_DET])\n",
    "    df[constants.Param_diff_SYM]=((df[constants.Param_tgt_SYM]-df[constants.Param_src_SYM])/df[constants.Param_tgt_SYM])\n",
    "    df[constants.Param_diff_PART]=((df[constants.Param_tgt_PART]-df[constants.Param_src_PART])/df[constants.Param_tgt_PART]).replace(np.NINF,0)\n",
    "    df[constants.Param_diff_PUNCT]=((df[constants.Param_tgt_PUNCT]-df[constants.Param_src_PUNCT])/df[constants.Param_tgt_PUNCT])\n",
    "    df[constants.Param_diff_countAbnormlWord]=(( df[constants.Param_tgt_countAbnormlWord]- df[constants.Param_src_countAbnormlWord])/ df[constants.Param_tgt_countAbnormlWord]).replace(np.Infinity,0)\n",
    "    df[constants.Param_diff_ratioAbnormal]=((df[constants.Param_tgt_ratioAbnormal]-df[constants.Param_src_ratioAbnormal])/df[constants.Param_tgt_ratioAbnormal])\n",
    "    df=df.fillna(0)\n",
    "    df=df.replace(np.Infinity,0)\n",
    "\n",
    "    return df\n",
    "def PreProcess_MaskModality(df):\n",
    "    \n",
    "    print('Starting preprocessing ....')\n",
    "    if df.isnull().sum().all().sum()>0:\n",
    "        print('\\n Data has some null values')\n",
    "        print('\\n Fill NaN value to 0')\n",
    "        df=df.fillna(0)\n",
    "        print('\\n done')\n",
    "        \n",
    "    print('\\n Create dummies with OneHot encoder for subjects and modalities')\n",
    "    translator='subject_id'\n",
    "    modality='modality'\n",
    "    \n",
    "    traslator_vectors = {}\n",
    "    #modality_vectors = {}\n",
    "\n",
    "    traslator_vectors = pd.get_dummies(df[translator])\n",
    "    #modality_vectors = pd.get_dummies(df[modality])\n",
    "    i=1\n",
    "    for subject in constants.subjects:\n",
    "        df[subject]=traslator_vectors[subject]\n",
    "\n",
    "    #for modality in constants.modalities:\n",
    "        #df[modality]=modality_vectors[modality]\n",
    "\n",
    "    print('done \\n Replace nan string with empty in aligned_edit and mt_text')\n",
    "    #replace nan string in aligned_edit and mt_text\n",
    "    #df[constants.Param_aligned_edit].replace('nan','',inplace=True)\n",
    "    #df[constants.Param_mt_text].replace('nan','',inplace=True)\n",
    "    \n",
    "    print('done \\n Creating new columns number of words and number of uniaue words for src_text, mt_text and tgt_text')\n",
    "    # Number of words in src_text,mt_text,tgt_text\n",
    "    #df[constants.Param_num_word_src] = df['src_text'].apply(lambda x: len(str(x).split()))\n",
    "    #df[constants.Param_num_word_mt] = df[constants.Param_mt_text].apply(lambda x: len(str(x).split()))\n",
    "    df[constants.Param_num_word_tgt] = df[constants.Param_tgt_text].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "    # Number of unigue words in src_text,mt_text,tgt_text\n",
    "    #df[\"num_unique_src\"] = df[\"src_text\"].apply(lambda x: len(set(str(x).split())))\n",
    "    #df[constants.Param_num_word_unique_mt] = df[constants.Param_mt_text].apply(lambda x: len(set(str(x).split())))\n",
    "    df[constants.Param_num_word_unique_tgt] = df[constants.Param_tgt_text].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "    # time to each word in the entire edit time\n",
    "    df[constants.Param_num_words_per_edit_time]=df[constants.Param_edit_time]/df[constants.Param_num_word_tgt]\n",
    "    \n",
    "    # Number of unigue words in mt_text group by subject \n",
    "    #constants.s_mt_t_wise=df.groupby(constants.Param_subject_id)[constants.Param_mt_text].agg(lambda x: ' '.join(set(x)))\n",
    "    #constants.s_mt_t_wise = constants.s_mt_t_wise.apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "    # Number of unigue words in tgt_text group by subject \n",
    "    constants.s_tgt_t_wise=df.groupby(constants.Param_subject_id)[constants.Param_tgt_text].agg(lambda x: ' '.join(set(x)))\n",
    "    constants.s_tgt_t_wise = constants.s_tgt_t_wise.apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "    # Number of unigue words in mt_text group by modality \n",
    "    #constants.s_mt_m_wise=df.groupby(constants.Param_modality)[constants.Param_mt_text].agg(lambda x: ' '.join(set(x)))\n",
    "    #constants.s_mt_m_wise = constants.s_mt_m_wise.apply(lambda x: len(set(str(x).split())))\n",
    "    \n",
    "    # Number of unigue words in tgt_text group by modality\n",
    "    #constants.s_tgt_m_wise=df.groupby(constants.Param_modality)[constants.Param_tgt_text].agg(lambda x: ' '.join(set(x)))\n",
    "    #constants.s_tgt_m_wise= constants.s_tgt_m_wise.apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "    # Calculating Ratio 300 & 1000\n",
    "    df[constants.Param_n300_ratio]=(df[constants.Param_n_pause_geq_300]/df[constants.Param_edit_time])*100\n",
    "    df[constants.Param_n1000_ratio]=(df[constants.Param_n_pause_geq_1000]/df[constants.Param_edit_time])*100\n",
    "\n",
    "    # Setting unique numbers(above values) in the dataframe\n",
    "    for subject in constants.subjects:\n",
    "        #df.loc[(df[constants.Param_subject_id] == subject), 'num_unique_mt_t_wise']=constants.s_mt_t_wise[subject]\n",
    "        df.loc[(df[constants.Param_subject_id] == subject), 'num_unique_tgt_t_wise']=constants.s_tgt_t_wise[subject]\n",
    "\n",
    "    #for modality in constants.modalities:\n",
    "        #df.loc[(df[constants.Param_modality] == modality), 'num_unique_mt_m_wise']=constants.s_mt_m_wise[modality]\n",
    "        #df.loc[(df[constants.Param_modality] == modality), 'num_unique_tgt_m_wise']=constants.s_tgt_m_wise[modality]\n",
    "    \n",
    "    # Aligned text extraction\n",
    "    \"\"\"\n",
    "    new= df[constants.Param_aligned_edit].str.split(\"EVAL:\", n = 1, expand = True)\n",
    "    df[constants.Param_seperated_aligned_edit] =new[1]\n",
    "    df[constants.Param_seperated_aligned_edit].replace('NONE','',inplace=True)\n",
    "    df[constants.Param_num_of_I] = df[constants.Param_seperated_aligned_edit].apply(lambda x: len(str(x).split('I'))-1)\n",
    "    df[constants.Param_num_of_D] = df[constants.Param_seperated_aligned_edit].apply(lambda x: len(str(x).split('D'))-1)\n",
    "    df[constants.Param_num_of_S] = df[constants.Param_seperated_aligned_edit].apply(lambda x: len(str(x).split('S'))-1)\n",
    "\n",
    "\n",
    "    df = df.drop(constants.Param_seperated_aligned_edit, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    #new_df[[constants.Param_subject_id,constants.Param_aligned_edit,constants.Param_num_of_I,constants.Param_num_of_D,constants.Param_num_of_S,constants.Param_num_of_IDS]]\n",
    "    \n",
    "    # linguistic src_text features + ratio Abnormal tokens\n",
    "    text=constants.Param_src_text\n",
    "    df[constants.Param_src_countWord3]=df[text].map( lambda x: countWords_Src(x,3))\n",
    "    df[constants.Param_src_countWord4]=df[text].map( lambda x: countWords_Src(x,4))\n",
    "    df[constants.Param_src_totalNumberOfTokens]=df[text].map( lambda x: getNumberOfTokens_Src(x))\n",
    "    df[constants.Param_src_NOUN]=df[text].map( lambda x: getCount_Src(x,[NOUN]))\n",
    "    df[constants.Param_src_VERB]=df[text].map( lambda x: getCount_Src(x,[VERB]))\n",
    "    df[constants.Param_src_ADJ]=df[text].map( lambda x: getCount_Src(x,[ADJ]))\n",
    "    df[constants.Param_src_ADV]=df[text].map( lambda x: getCount_Src(x,[ADV]))\n",
    "    df[constants.Param_src_PROPN]=df[text].map( lambda x: getCount_Src(x,[PROPN]))\n",
    "    df[constants.Param_src_PRON]=df[text].map( lambda x: getCount_Src(x,[PRON]))\n",
    "    df[constants.Param_src_AUX]=df[text].map( lambda x: getCount_Src(x,[AUX]))\n",
    "    df[constants.Param_src_CONJ]=df[text].map( lambda x: getCount_Src(x,[CONJ,CCONJ,SCONJ]))\n",
    "    df[constants.Param_src_ADP]=df[text].map( lambda x: getCount_Src(x,[ADP]))\n",
    "    df[constants.Param_src_DET]=df[text].map( lambda x: getCount_Src(x,[DET]))\n",
    "    df[constants.Param_src_SYM]=df[text].map( lambda x: getCount_Src(x,[SYM]))\n",
    "    df[constants.Param_src_PART]=df[text].map( lambda x: getCount_Src(x,[PART]))\n",
    "    df[constants.Param_src_PUNCT]=df[text].map( lambda x: getCount_Src(x,[PUNCT]))\n",
    "    df[constants.Param_src_countAbnormlWord]=(df[constants.Param_src_countWord4]+df[constants.Param_src_countWord3])-(df[constants.Param_src_DET]+df[constants.Param_src_PUNCT]+df[constants.Param_src_ADP])\n",
    "    \n",
    "    df[constants.Param_src_ratioAbnormal]=(df[constants.Param_src_countAbnormlWord]/df[constants.Param_src_totalNumberOfTokens])*100\n",
    "    df[constants.Param_src_ratioAbnormal]=df[constants.Param_src_ratioAbnormal].replace(np.Infinity,0)\n",
    "   \n",
    "                                          \n",
    "\n",
    "\n",
    "    # linguistic tgt_text features + ratio Abnormal tokens\n",
    "    text=constants.Param_tgt_text\n",
    "    df[constants.Param_tgt_countWord3]=df[text].map( lambda x: countWords(x,3))\n",
    "    df[constants.Param_tgt_countWord4]=df[text].map( lambda x: countWords(x,4))\n",
    "    df[constants.Param_tgt_totalNumberOfTokens]=df[text].map( lambda x: getNumberOfTokens(x))\n",
    "    df[constants.Param_tgt_NOUN]=df[text].map( lambda x: getCount(x,[NOUN]))\n",
    "    df[constants.Param_tgt_VERB]=df[text].map( lambda x: getCount(x,[VERB]))\n",
    "    df[constants.Param_tgt_ADJ]=df[text].map( lambda x: getCount(x,[ADJ]))\n",
    "    df[constants.Param_tgt_ADV]=df[text].map( lambda x: getCount(x,[ADV]))\n",
    "    df[constants.Param_tgt_PROPN]=df[text].map( lambda x: getCount(x,[PROPN]))\n",
    "    df[constants.Param_tgt_PRON]=df[text].map( lambda x: getCount(x,[PRON]))\n",
    "    df[constants.Param_tgt_AUX]=df[text].map( lambda x: getCount(x,[AUX]))\n",
    "    df[constants.Param_tgt_CONJ]=df[text].map( lambda x: getCount(x,[CONJ,CCONJ,SCONJ]))\n",
    "    df[constants.Param_tgt_ADP]=df[text].map( lambda x: getCount(x,[ADP]))\n",
    "    df[constants.Param_tgt_DET]=df[text].map( lambda x: getCount(x,[DET]))\n",
    "    df[constants.Param_tgt_SYM]=df[text].map( lambda x: getCount(x,[SYM]))\n",
    "    df[constants.Param_tgt_PART]=df[text].map( lambda x: getCount(x,[PART]))\n",
    "    df[constants.Param_tgt_PUNCT]=df[text].map( lambda x: getCount(x,[PUNCT]))\n",
    "    df[constants.Param_tgt_countAbnormlWord]=(df[constants.Param_tgt_countWord4]+df[constants.Param_tgt_countWord3])-(df[constants.Param_tgt_DET]+df[constants.Param_tgt_PUNCT]+df[constants.Param_tgt_ADP])\n",
    "    df[constants.Param_tgt_ratioAbnormal]=(df[constants.Param_tgt_countAbnormlWord]/df[constants.Param_tgt_totalNumberOfTokens])*100\n",
    "    df[constants.Param_tgt_ratioAbnormal]=df[constants.Param_tgt_ratioAbnormal]\n",
    "    # linguistic featurs diff \n",
    "    df[constants.Param_diff_countWord3]=((df[constants.Param_tgt_countWord3]-df[constants.Param_src_countWord3])/df[constants.Param_tgt_countWord3])\n",
    "    df[constants.Param_diff_countWord4]=((df[constants.Param_tgt_countWord4]-df[constants.Param_src_countWord4])/df[constants.Param_tgt_countWord4])\n",
    "    df[constants.Param_diff_totalNumberOfTokens]=((df[constants.Param_tgt_totalNumberOfTokens]-df[constants.Param_src_totalNumberOfTokens])/df[constants.Param_tgt_totalNumberOfTokens])\n",
    "    df[constants.Param_diff_NOUN]=((df[constants.Param_tgt_NOUN]-df[constants.Param_src_NOUN])/df[constants.Param_tgt_NOUN])\n",
    "    df[constants.Param_diff_VERB]=((df[constants.Param_tgt_VERB]-df[constants.Param_src_VERB])/df[constants.Param_tgt_VERB])\n",
    "    df[constants.Param_diff_ADJ]=((df[constants.Param_tgt_ADJ]-df[constants.Param_src_ADJ])/df[constants.Param_tgt_ADJ])\n",
    "    df[constants.Param_diff_ADV]=((df[constants.Param_tgt_ADV]-df[constants.Param_src_ADV])/df[constants.Param_tgt_ADV])\n",
    "    df[constants.Param_diff_PROPN]=((df[constants.Param_tgt_PROPN]-df[constants.Param_src_PROPN])/df[constants.Param_tgt_PROPN])\n",
    "    df[constants.Param_diff_PRON]=((df[constants.Param_tgt_PRON]-df[constants.Param_src_PRON])/df[constants.Param_tgt_PRON])\n",
    "    df[constants.Param_diff_AUX]=((df[constants.Param_tgt_AUX]-df[constants.Param_src_AUX])/df[constants.Param_tgt_AUX])\n",
    "    df[constants.Param_diff_CONJ]=((df[constants.Param_tgt_CONJ]-df[constants.Param_src_CONJ])/df[constants.Param_tgt_CONJ])\n",
    "    df[constants.Param_diff_ADP]=((df[constants.Param_tgt_ADP]-df[constants.Param_src_ADP])/df[constants.Param_tgt_ADP])\n",
    "    df[constants.Param_diff_DET]=((df[constants.Param_tgt_DET]-df[constants.Param_src_DET])/df[constants.Param_tgt_DET])\n",
    "    df[constants.Param_diff_SYM]=((df[constants.Param_tgt_SYM]-df[constants.Param_src_SYM])/df[constants.Param_tgt_SYM])\n",
    "    df[constants.Param_diff_PART]=((df[constants.Param_tgt_PART]-df[constants.Param_src_PART])/df[constants.Param_tgt_PART]).replace(np.NINF,0)\n",
    "    df[constants.Param_diff_PUNCT]=((df[constants.Param_tgt_PUNCT]-df[constants.Param_src_PUNCT])/df[constants.Param_tgt_PUNCT])\n",
    "    df[constants.Param_diff_countAbnormlWord]=(( df[constants.Param_tgt_countAbnormlWord]- df[constants.Param_src_countAbnormlWord])/ df[constants.Param_tgt_countAbnormlWord]).replace(np.Infinity,0)\n",
    "    df[constants.Param_diff_ratioAbnormal]=((df[constants.Param_tgt_ratioAbnormal]-df[constants.Param_src_ratioAbnormal])/df[constants.Param_tgt_ratioAbnormal])\n",
    "    df=df.fillna(0)\n",
    "    df=df.replace(np.Infinity,0)\n",
    "    return df\n",
    "def PreProcess_MaskTime(df):\n",
    "    \n",
    "    print('Starting preprocessing ....')\n",
    "    if df.isnull().sum().all().sum()>0:\n",
    "        print('\\n Data has some null values')\n",
    "        print('\\n Fill NaN value to 0')\n",
    "        df=df.fillna(0)\n",
    "        print('\\n done')\n",
    "        \n",
    "    print('\\n Create dummies with OneHot encoder for subjects and modalities')\n",
    "    translator='subject_id'\n",
    "    modality='modality'\n",
    "    \n",
    "    traslator_vectors = {}\n",
    "    modality_vectors = {}\n",
    "    i=1\n",
    "    modality_vectors = pd.get_dummies(df[modality])\n",
    "    traslator_vectors = pd.get_dummies(df[translator])\n",
    "\n",
    "    for subject in constants.subjects:\n",
    "        df[subject]=traslator_vectors[subject]\n",
    "\n",
    "    for modality in constants.modalities:\n",
    "        df[modality]=modality_vectors[modality]\n",
    "\n",
    "    print('done \\n Replace nan string with empty in aligned_edit and mt_text')\n",
    "    #replace nan string in aligned_edit and mt_text\n",
    "    df[constants.Param_aligned_edit].replace('nan','',inplace=True)\n",
    "    df[constants.Param_mt_text].replace('nan','',inplace=True)\n",
    "    \n",
    "    print('done \\n Creating new columns number of words and number of uniaue words for src_text, mt_text and tgt_text')\n",
    "    # Number of words in src_text,mt_text,tgt_text\n",
    "    df[constants.Param_num_word_src] = df['src_text'].apply(lambda x: len(str(x).split()))\n",
    "    df[constants.Param_num_word_mt] = df[constants.Param_mt_text].apply(lambda x: len(str(x).split()))\n",
    "    df[constants.Param_num_word_tgt] = df[constants.Param_tgt_text].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "    \n",
    "\n",
    "    # Number of unigue words in src_text,mt_text,tgt_text\n",
    "    #df[\"num_unique_src\"] = df[\"src_text\"].apply(lambda x: len(set(str(x).split())))\n",
    "    df[constants.Param_num_word_unique_mt] = df[constants.Param_mt_text].apply(lambda x: len(set(str(x).split())))\n",
    "    df[constants.Param_num_word_unique_tgt] = df[constants.Param_tgt_text].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "    \n",
    "    # Number of unigue words in mt_text group by subject \n",
    "    constants.s_mt_t_wise=df.groupby(constants.Param_subject_id)[constants.Param_mt_text].agg(lambda x: ' '.join(set(x)))\n",
    "    constants.s_mt_t_wise = constants.s_mt_t_wise.apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "    # Number of unigue words in tgt_text group by subject \n",
    "    constants.s_tgt_t_wise=df.groupby(constants.Param_subject_id)[constants.Param_tgt_text].agg(lambda x: ' '.join(set(x)))\n",
    "    constants.s_tgt_t_wise = constants.s_tgt_t_wise.apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "    # Number of unigue words in mt_text group by modality \n",
    "    constants.s_mt_m_wise=df.groupby(constants.Param_modality)[constants.Param_mt_text].agg(lambda x: ' '.join(set(x)))\n",
    "    constants.s_mt_m_wise = constants.s_mt_m_wise.apply(lambda x: len(set(str(x).split())))\n",
    "    \n",
    "    # Number of unigue words in tgt_text group by modality\n",
    "    constants.s_tgt_m_wise=df.groupby(constants.Param_modality)[constants.Param_tgt_text].agg(lambda x: ' '.join(set(x)))\n",
    "    constants.s_tgt_m_wise= constants.s_tgt_m_wise.apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "    # Calculating Ratio 300 & 1000\n",
    "    #df[constants.Param_n300_ratio]=(df[constants.Param_n_pause_geq_300]/df[constants.Param_edit_time])*100\n",
    "    #df[constants.Param_n1000_ratio]=(df[constants.Param_n_pause_geq_1000]/df[constants.Param_edit_time])*100\n",
    "\n",
    "    # Setting unique numbers(above values) in the dataframe\n",
    "    for subject in constants.subjects:\n",
    "        df.loc[(df[constants.Param_subject_id] == subject), 'num_unique_mt_t_wise']=constants.s_mt_t_wise[subject]\n",
    "        df.loc[(df[constants.Param_subject_id] == subject), 'num_unique_tgt_t_wise']=constants.s_tgt_t_wise[subject]\n",
    "\n",
    "    for modality in constants.modalities:\n",
    "        df.loc[(df[constants.Param_modality] == modality), 'num_unique_mt_m_wise']=constants.s_mt_m_wise[modality]\n",
    "        df.loc[(df[constants.Param_modality] == modality), 'num_unique_tgt_m_wise']=constants.s_tgt_m_wise[modality]\n",
    "    \n",
    "    # Aligned text extraction\n",
    "    new= df[constants.Param_aligned_edit].str.split(\"EVAL:\", n = 1, expand = True)\n",
    "    df[constants.Param_seperated_aligned_edit] =new[1]\n",
    "    df[constants.Param_seperated_aligned_edit].replace('NONE','',inplace=True)\n",
    "    df[constants.Param_num_of_I] = df[constants.Param_seperated_aligned_edit].apply(lambda x: len(str(x).split('I'))-1)\n",
    "    df[constants.Param_num_of_D] = df[constants.Param_seperated_aligned_edit].apply(lambda x: len(str(x).split('D'))-1)\n",
    "    df[constants.Param_num_of_S] = df[constants.Param_seperated_aligned_edit].apply(lambda x: len(str(x).split('S'))-1)\n",
    "    df[constants.Param_num_of_IDS] = df[constants.Param_num_of_I]+df[constants.Param_num_of_D]+df[constants.Param_num_of_S]\n",
    "\n",
    "    #new_df[[constants.Param_subject_id,constants.Param_aligned_edit,constants.Param_num_of_I,constants.Param_num_of_D,constants.Param_num_of_S,constants.Param_num_of_IDS]]\n",
    "    \n",
    "    df = df.drop(constants.Param_seperated_aligned_edit, 1)\n",
    "\n",
    "    # linguistic src_text features + ratio Abnormal tokens\n",
    "    text=constants.Param_src_text\n",
    "    df[constants.Param_src_countWord3]=df[text].map( lambda x: countWords_Src(x,3))\n",
    "    df[constants.Param_src_countWord4]=df[text].map( lambda x: countWords_Src(x,4))\n",
    "    df[constants.Param_src_totalNumberOfTokens]=df[text].map( lambda x: getNumberOfTokens_Src(x))\n",
    "    df[constants.Param_src_NOUN]=df[text].map( lambda x: getCount_Src(x,[NOUN]))\n",
    "    df[constants.Param_src_VERB]=df[text].map( lambda x: getCount_Src(x,[VERB]))\n",
    "    df[constants.Param_src_ADJ]=df[text].map( lambda x: getCount_Src(x,[ADJ]))\n",
    "    df[constants.Param_src_ADV]=df[text].map( lambda x: getCount_Src(x,[ADV]))\n",
    "    df[constants.Param_src_PROPN]=df[text].map( lambda x: getCount_Src(x,[PROPN]))\n",
    "    df[constants.Param_src_PRON]=df[text].map( lambda x: getCount_Src(x,[PRON]))\n",
    "    df[constants.Param_src_AUX]=df[text].map( lambda x: getCount_Src(x,[AUX]))\n",
    "    df[constants.Param_src_CONJ]=df[text].map( lambda x: getCount_Src(x,[CONJ,CCONJ,SCONJ]))\n",
    "    df[constants.Param_src_ADP]=df[text].map( lambda x: getCount_Src(x,[ADP]))\n",
    "    df[constants.Param_src_DET]=df[text].map( lambda x: getCount_Src(x,[DET]))\n",
    "    df[constants.Param_src_SYM]=df[text].map( lambda x: getCount_Src(x,[SYM]))\n",
    "    df[constants.Param_src_PART]=df[text].map( lambda x: getCount_Src(x,[PART]))\n",
    "    df[constants.Param_src_PUNCT]=df[text].map( lambda x: getCount_Src(x,[PUNCT]))\n",
    "    df[constants.Param_src_countAbnormlWord]=(df[constants.Param_src_countWord4]+df[constants.Param_src_countWord3])-(df[constants.Param_src_DET]+df[constants.Param_src_PUNCT]+df[constants.Param_src_ADP])\n",
    "    \n",
    "    df[constants.Param_src_ratioAbnormal]=(df[constants.Param_src_countAbnormlWord]/df[constants.Param_src_totalNumberOfTokens])*100\n",
    "    df[constants.Param_src_ratioAbnormal]=df[constants.Param_src_ratioAbnormal].replace(np.Infinity,0)\n",
    "   \n",
    "                                          \n",
    "\n",
    "\n",
    "    # linguistic tgt_text features + ratio Abnormal tokens\n",
    "    text=constants.Param_tgt_text\n",
    "    df[constants.Param_tgt_countWord3]=df[text].map( lambda x: countWords(x,3))\n",
    "    df[constants.Param_tgt_countWord4]=df[text].map( lambda x: countWords(x,4))\n",
    "    df[constants.Param_tgt_totalNumberOfTokens]=df[text].map( lambda x: getNumberOfTokens(x))\n",
    "    df[constants.Param_tgt_NOUN]=df[text].map( lambda x: getCount(x,[NOUN]))\n",
    "    df[constants.Param_tgt_VERB]=df[text].map( lambda x: getCount(x,[VERB]))\n",
    "    df[constants.Param_tgt_ADJ]=df[text].map( lambda x: getCount(x,[ADJ]))\n",
    "    df[constants.Param_tgt_ADV]=df[text].map( lambda x: getCount(x,[ADV]))\n",
    "    df[constants.Param_tgt_PROPN]=df[text].map( lambda x: getCount(x,[PROPN]))\n",
    "    df[constants.Param_tgt_PRON]=df[text].map( lambda x: getCount(x,[PRON]))\n",
    "    df[constants.Param_tgt_AUX]=df[text].map( lambda x: getCount(x,[AUX]))\n",
    "    df[constants.Param_tgt_CONJ]=df[text].map( lambda x: getCount(x,[CONJ,CCONJ,SCONJ]))\n",
    "    df[constants.Param_tgt_ADP]=df[text].map( lambda x: getCount(x,[ADP]))\n",
    "    df[constants.Param_tgt_DET]=df[text].map( lambda x: getCount(x,[DET]))\n",
    "    df[constants.Param_tgt_SYM]=df[text].map( lambda x: getCount(x,[SYM]))\n",
    "    df[constants.Param_tgt_PART]=df[text].map( lambda x: getCount(x,[PART]))\n",
    "    df[constants.Param_tgt_PUNCT]=df[text].map( lambda x: getCount(x,[PUNCT]))\n",
    "    df[constants.Param_tgt_countAbnormlWord]=(df[constants.Param_tgt_countWord4]+df[constants.Param_tgt_countWord3])-(df[constants.Param_tgt_DET]+df[constants.Param_tgt_PUNCT]+df[constants.Param_tgt_ADP])\n",
    "    df[constants.Param_tgt_ratioAbnormal]=(df[constants.Param_tgt_countAbnormlWord]/df[constants.Param_tgt_totalNumberOfTokens])*100\n",
    "    df[constants.Param_tgt_ratioAbnormal]=df[constants.Param_tgt_ratioAbnormal]\n",
    "    # linguistic featurs diff \n",
    "    df[constants.Param_diff_countWord3]=((df[constants.Param_tgt_countWord3]-df[constants.Param_src_countWord3])/df[constants.Param_tgt_countWord3])\n",
    "    df[constants.Param_diff_countWord4]=((df[constants.Param_tgt_countWord4]-df[constants.Param_src_countWord4])/df[constants.Param_tgt_countWord4])\n",
    "    df[constants.Param_diff_totalNumberOfTokens]=((df[constants.Param_tgt_totalNumberOfTokens]-df[constants.Param_src_totalNumberOfTokens])/df[constants.Param_tgt_totalNumberOfTokens])\n",
    "    df[constants.Param_diff_NOUN]=((df[constants.Param_tgt_NOUN]-df[constants.Param_src_NOUN])/df[constants.Param_tgt_NOUN])\n",
    "    df[constants.Param_diff_VERB]=((df[constants.Param_tgt_VERB]-df[constants.Param_src_VERB])/df[constants.Param_tgt_VERB])\n",
    "    df[constants.Param_diff_ADJ]=((df[constants.Param_tgt_ADJ]-df[constants.Param_src_ADJ])/df[constants.Param_tgt_ADJ])\n",
    "    df[constants.Param_diff_ADV]=((df[constants.Param_tgt_ADV]-df[constants.Param_src_ADV])/df[constants.Param_tgt_ADV])\n",
    "    df[constants.Param_diff_PROPN]=((df[constants.Param_tgt_PROPN]-df[constants.Param_src_PROPN])/df[constants.Param_tgt_PROPN])\n",
    "    df[constants.Param_diff_PRON]=((df[constants.Param_tgt_PRON]-df[constants.Param_src_PRON])/df[constants.Param_tgt_PRON])\n",
    "    df[constants.Param_diff_AUX]=((df[constants.Param_tgt_AUX]-df[constants.Param_src_AUX])/df[constants.Param_tgt_AUX])\n",
    "    df[constants.Param_diff_CONJ]=((df[constants.Param_tgt_CONJ]-df[constants.Param_src_CONJ])/df[constants.Param_tgt_CONJ])\n",
    "    df[constants.Param_diff_ADP]=((df[constants.Param_tgt_ADP]-df[constants.Param_src_ADP])/df[constants.Param_tgt_ADP])\n",
    "    df[constants.Param_diff_DET]=((df[constants.Param_tgt_DET]-df[constants.Param_src_DET])/df[constants.Param_tgt_DET])\n",
    "    df[constants.Param_diff_SYM]=((df[constants.Param_tgt_SYM]-df[constants.Param_src_SYM])/df[constants.Param_tgt_SYM])\n",
    "    df[constants.Param_diff_PART]=((df[constants.Param_tgt_PART]-df[constants.Param_src_PART])/df[constants.Param_tgt_PART]).replace(np.NINF,0)\n",
    "    df[constants.Param_diff_PUNCT]=((df[constants.Param_tgt_PUNCT]-df[constants.Param_src_PUNCT])/df[constants.Param_tgt_PUNCT])\n",
    "    df[constants.Param_diff_countAbnormlWord]=(( df[constants.Param_tgt_countAbnormlWord]- df[constants.Param_src_countAbnormlWord])/ df[constants.Param_tgt_countAbnormlWord]).replace(np.Infinity,0)\n",
    "    df[constants.Param_diff_ratioAbnormal]=((df[constants.Param_tgt_ratioAbnormal]-df[constants.Param_src_ratioAbnormal])/df[constants.Param_tgt_ratioAbnormal])\n",
    "    df=df.fillna(0)\n",
    "    df=df.replace(np.Infinity,0)\n",
    "    return df\n",
    "\n",
    "def createMaindataFrame(odf):\n",
    "    odf=PreProcess(odf)\n",
    "    odf=odf.fillna(0)\n",
    "    odf=odf.replace(np.Infinity,0)\n",
    "\n",
    "    print('\\n Creating main data frame ...')\n",
    "    dfSubject=[]\n",
    "    columns=[\n",
    "        constants.Param_translator1,constants.Param_translator2,constants.Param_translator3,\n",
    "        constants.Param_modality_ht,constants.Param_modality_pe1,constants.Param_modality_pe2,\n",
    "        constants.Param_k_total,constants.Param_k_letter,constants.Param_k_copy,constants.Param_k_cut,constants.Param_k_digit,constants.Param_k_erase,\n",
    "        constants.Param_k_nav,constants.Param_k_paste,constants.Param_k_symbol,constants.Param_k_white,\n",
    "        constants.Param_n_delete,constants.Param_n_insert,constants.Param_n_shift,constants.Param_n_substitute,constants.Param_num_annotations,\n",
    "        constants.Param_n_pause_geq_300,constants.Param_len_pause_geq_300,constants.Param_n_pause_geq_1000,constants.Param_len_pause_geq_1000,\n",
    "        constants.Param_TER,constants.Param_BLEU,constants.Param_CHRF,\n",
    "        constants.Param_n300_ratio,constants.Param_n1000_ratio,\n",
    "        constants.Param_num_word_mt,constants.Param_num_word_tgt,constants.Param_num_words_per_edit_time,\n",
    "        constants.Param_num_word_unique_mt,constants.Param_num_word_unique_tgt,\n",
    "        constants.Param_num_unique_mt_t_wise,constants.Param_num_unique_tgt_t_wise,\n",
    "        constants.Param_num_unique_mt_m_wise,constants.Param_num_unique_tgt_m_wise,\n",
    "        constants.Param_num_of_I,constants.Param_num_of_D,constants.Param_num_of_S,constants.Param_num_of_IDS,\n",
    "        \n",
    "        constants.Param_src_NOUN,constants.Param_src_PROPN,constants.Param_src_VERB,constants.Param_src_ADJ,constants.Param_src_ADV,constants.Param_src_ADP,\n",
    "        constants.Param_src_CONJ,constants.Param_src_PUNCT,constants.Param_src_DET,constants.Param_src_PRON,constants.Param_src_AUX,constants.Param_src_SYM,\n",
    "        constants.Param_src_PART,\n",
    "        constants.Param_src_countAbnormlWord,constants.Param_src_totalNumberOfTokens,constants.Param_src_ratioAbnormal,\n",
    "\n",
    "        constants.Param_tgt_NOUN,constants.Param_tgt_PROPN,constants.Param_tgt_VERB,constants.Param_tgt_ADJ,constants.Param_tgt_ADV,constants.Param_tgt_ADP,\n",
    "        constants.Param_tgt_CONJ,constants.Param_tgt_PUNCT,constants.Param_tgt_DET,constants.Param_tgt_PRON,constants.Param_tgt_AUX,constants.Param_tgt_SYM,\n",
    "        constants.Param_tgt_PART,\n",
    "        constants.Param_tgt_countAbnormlWord,constants.Param_tgt_totalNumberOfTokens,constants.Param_tgt_ratioAbnormal,\n",
    "\n",
    "        constants.Param_diff_NOUN,constants.Param_diff_PROPN,constants.Param_diff_VERB,constants.Param_diff_ADJ,constants.Param_diff_ADV,constants.Param_diff_ADP,\n",
    "        constants.Param_diff_CONJ,constants.Param_diff_PUNCT,constants.Param_diff_DET,constants.Param_diff_PRON,constants.Param_diff_AUX,constants.Param_diff_SYM,\n",
    "        constants.Param_diff_PART,\n",
    "        constants.Param_diff_countAbnormlWord,constants.Param_diff_totalNumberOfTokens,constants.Param_diff_ratioAbnormal\n",
    "\n",
    "\n",
    "        ]\n",
    "    dfSubject=odf[columns].copy()\n",
    "    return dfSubject\n",
    "def createMaskSubjectDataFrame(odf):\n",
    "    odf=PreProcess_MaskSubject(odf)\n",
    "    odf=odf.fillna(0)\n",
    "    print('\\n Creating mask subject data frame ...')\n",
    "    dfSubject=[]\n",
    "    columns=[\n",
    "        constants.Param_modality_ht,constants.Param_modality_pe1,constants.Param_modality_pe2,\n",
    "        constants.Param_edit_time,\n",
    "        constants.Param_k_total,constants.Param_k_letter,constants.Param_k_copy,constants.Param_k_cut,constants.Param_k_digit,constants.Param_k_erase,\n",
    "        constants.Param_k_nav,constants.Param_k_paste,constants.Param_k_symbol,constants.Param_k_white,\n",
    "        constants.Param_n_delete,constants.Param_n_insert,constants.Param_n_shift,constants.Param_n_substitute,constants.Param_num_annotations,\n",
    "        constants.Param_n_pause_geq_300,constants.Param_len_pause_geq_300,constants.Param_n_pause_geq_1000,\n",
    "        constants.Param_len_pause_geq_1000,constants.Param_TER,constants.Param_BLEU,constants.Param_CHRF,\n",
    "        constants.Param_n300_ratio,constants.Param_n1000_ratio,\n",
    "        constants.Param_num_word_mt,constants.Param_num_word_tgt,constants.Param_num_words_per_edit_time,\n",
    "        constants.Param_num_word_unique_mt,constants.Param_num_word_unique_tgt,\n",
    "        constants.Param_num_unique_mt_m_wise,constants.Param_num_unique_tgt_m_wise,\n",
    "        constants.Param_num_of_I,constants.Param_num_of_D,constants.Param_num_of_S,constants.Param_num_of_IDS,\n",
    "        constants.Param_src_NOUN,constants.Param_src_PROPN,constants.Param_src_VERB,constants.Param_src_ADJ,constants.Param_src_ADV,constants.Param_src_ADP,\n",
    "        constants.Param_src_CONJ,constants.Param_src_PUNCT,constants.Param_src_DET,constants.Param_src_PRON,constants.Param_src_AUX,constants.Param_src_SYM,\n",
    "        constants.Param_src_PART,\n",
    "        constants.Param_src_countAbnormlWord,constants.Param_src_totalNumberOfTokens,constants.Param_src_ratioAbnormal,\n",
    "\n",
    "        constants.Param_tgt_NOUN,constants.Param_tgt_PROPN,constants.Param_tgt_VERB,constants.Param_tgt_ADJ,constants.Param_tgt_ADV,constants.Param_tgt_ADP,\n",
    "        constants.Param_tgt_CONJ,constants.Param_tgt_PUNCT,constants.Param_tgt_DET,constants.Param_tgt_PRON,constants.Param_tgt_AUX,constants.Param_tgt_SYM,\n",
    "        constants.Param_tgt_PART,\n",
    "        constants.Param_tgt_countAbnormlWord,constants.Param_tgt_totalNumberOfTokens,constants.Param_tgt_ratioAbnormal,\n",
    "\n",
    "        constants.Param_diff_NOUN,constants.Param_diff_PROPN,constants.Param_diff_VERB,constants.Param_diff_ADJ,constants.Param_diff_ADV,constants.Param_diff_ADP,\n",
    "        constants.Param_diff_CONJ,constants.Param_diff_PUNCT,constants.Param_diff_DET,constants.Param_diff_PRON,constants.Param_diff_AUX,constants.Param_diff_SYM,\n",
    "        constants.Param_diff_PART,\n",
    "        constants.Param_diff_countAbnormlWord,constants.Param_diff_totalNumberOfTokens,constants.Param_diff_ratioAbnormal\n",
    "        ]\n",
    "    dfSubject=odf[columns].copy()\n",
    "    return dfSubject\n",
    "def createMaskModalityDataFrame(odf):\n",
    "    odf=PreProcess_MaskModality(odf)\n",
    "    odf=odf.fillna(0)\n",
    "    print('\\n Creating mask modality data frame ...')\n",
    "    odf.isnull().sum()\n",
    "    print('done \\n data frame has been created!')\n",
    "    \n",
    "    columns=[\n",
    "        constants.Param_translator1,constants.Param_translator2,constants.Param_translator3,\n",
    "        constants.Param_edit_time,\n",
    "        constants.Param_k_total,constants.Param_k_letter,constants.Param_k_copy,constants.Param_k_cut,constants.Param_k_digit,constants.Param_k_erase,\n",
    "        constants.Param_k_nav,constants.Param_k_paste,constants.Param_k_symbol,constants.Param_k_white,\n",
    "        #constants.Param_n_delete,constants.Param_n_insert,constants.Param_n_shift,constants.Param_n_substitute,\n",
    "        constants.Param_num_annotations,\n",
    "        constants.Param_n_pause_geq_300,constants.Param_len_pause_geq_300,constants.Param_n_pause_geq_1000,constants.Param_len_pause_geq_1000,\n",
    "        #constants.Param_TER,constants.Param_BLEU,constants.Param_CHRF,\n",
    "        constants.Param_n300_ratio,constants.Param_n1000_ratio,\n",
    "        #constants.Param_num_word_mt,\n",
    "        constants.Param_num_word_tgt,constants.Param_num_words_per_edit_time,\n",
    "        #constants.Param_num_word_unique_mt,\n",
    "        constants.Param_num_word_unique_tgt,\n",
    "        #constants.Param_num_unique_mt_t_wise,\n",
    "        constants.Param_num_unique_tgt_t_wise,\n",
    "        #,constants.Param_num_of_I,constants.Param_num_of_D,constants.Param_num_of_S,constants.Param_num_of_IDS\n",
    "        constants.Param_src_NOUN,constants.Param_src_PROPN,constants.Param_src_VERB,constants.Param_src_ADJ,constants.Param_src_ADV,constants.Param_src_ADP,\n",
    "        constants.Param_src_CONJ,constants.Param_src_PUNCT,constants.Param_src_DET,constants.Param_src_PRON,constants.Param_src_AUX,constants.Param_src_SYM,\n",
    "        constants.Param_src_PART,\n",
    "        constants.Param_src_countAbnormlWord,constants.Param_src_totalNumberOfTokens,constants.Param_src_ratioAbnormal,\n",
    "\n",
    "        constants.Param_tgt_NOUN,constants.Param_tgt_PROPN,constants.Param_tgt_VERB,constants.Param_tgt_ADJ,constants.Param_tgt_ADV,constants.Param_tgt_ADP,\n",
    "        constants.Param_tgt_CONJ,constants.Param_tgt_PUNCT,constants.Param_tgt_DET,constants.Param_tgt_PRON,constants.Param_tgt_AUX,constants.Param_tgt_SYM,\n",
    "        constants.Param_tgt_PART,\n",
    "        constants.Param_tgt_countAbnormlWord,constants.Param_tgt_totalNumberOfTokens,constants.Param_tgt_ratioAbnormal,\n",
    "\n",
    "        constants.Param_diff_NOUN,constants.Param_diff_PROPN,constants.Param_diff_VERB,constants.Param_diff_ADJ,constants.Param_diff_ADV,constants.Param_diff_ADP,\n",
    "        constants.Param_diff_CONJ,constants.Param_diff_PUNCT,constants.Param_diff_DET,constants.Param_diff_PRON,constants.Param_diff_AUX,constants.Param_diff_SYM,\n",
    "        constants.Param_diff_PART,\n",
    "        constants.Param_diff_countAbnormlWord,constants.Param_diff_totalNumberOfTokens,constants.Param_diff_ratioAbnormal\n",
    "        ]\n",
    "    dfModalities=odf[columns].copy()\n",
    "    return dfModalities   \n",
    "def createMaskTimeDataFrame(odf):\n",
    "    odf=PreProcess_MaskTime(odf)\n",
    "    odf=odf.fillna(0)\n",
    "    print('\\n Creating mask time data frame ...')\n",
    "\n",
    "    dfTime=[]\n",
    "    columns=[\n",
    "        constants.Param_translator1,constants.Param_translator2,constants.Param_translator3,\n",
    "        constants.Param_modality_ht,constants.Param_modality_pe1,constants.Param_modality_pe2,\n",
    "        constants.Param_k_total,constants.Param_k_letter,constants.Param_k_copy,constants.Param_k_cut,constants.Param_k_digit,constants.Param_k_erase,\n",
    "        constants.Param_k_nav,constants.Param_k_paste,constants.Param_k_symbol,constants.Param_k_white,\n",
    "        constants.Param_n_delete,constants.Param_n_insert,constants.Param_n_shift,constants.Param_n_substitute,constants.Param_num_annotations,\n",
    "        #constants.Param_n_pause_geq_300,constants.Param_len_pause_geq_300,constants.Param_n_pause_geq_1000,constants.Param_len_pause_geq_1000,\n",
    "        constants.Param_TER,constants.Param_BLEU,constants.Param_CHRF,\n",
    "        #constants.Param_n300_ratio,constants.Param_n1000_ratio,\n",
    "        constants.Param_num_word_mt,constants.Param_num_word_tgt,\n",
    "        constants.Param_num_word_unique_mt,constants.Param_num_word_unique_tgt,\n",
    "        constants.Param_num_unique_mt_t_wise,constants.Param_num_unique_tgt_t_wise,\n",
    "        constants.Param_num_unique_mt_m_wise,constants.Param_num_unique_tgt_m_wise,\n",
    "        constants.Param_num_of_I,constants.Param_num_of_D,constants.Param_num_of_S,constants.Param_num_of_IDS,\n",
    "        constants.Param_src_NOUN,constants.Param_src_PROPN,constants.Param_src_VERB,constants.Param_src_ADJ,constants.Param_src_ADV,constants.Param_src_ADP,\n",
    "        constants.Param_src_CONJ,constants.Param_src_PUNCT,constants.Param_src_DET,constants.Param_src_PRON,constants.Param_src_AUX,constants.Param_src_SYM,\n",
    "        constants.Param_src_PART,\n",
    "        constants.Param_src_countAbnormlWord,constants.Param_src_totalNumberOfTokens,constants.Param_src_ratioAbnormal,\n",
    "\n",
    "        constants.Param_tgt_NOUN,constants.Param_tgt_PROPN,constants.Param_tgt_VERB,constants.Param_tgt_ADJ,constants.Param_tgt_ADV,constants.Param_tgt_ADP,\n",
    "        constants.Param_tgt_CONJ,constants.Param_tgt_PUNCT,constants.Param_tgt_DET,constants.Param_tgt_PRON,constants.Param_tgt_AUX,constants.Param_tgt_SYM,\n",
    "        constants.Param_tgt_PART,\n",
    "        constants.Param_tgt_countAbnormlWord,constants.Param_tgt_totalNumberOfTokens,constants.Param_tgt_ratioAbnormal,\n",
    "\n",
    "        constants.Param_diff_NOUN,constants.Param_diff_PROPN,constants.Param_diff_VERB,constants.Param_diff_ADJ,constants.Param_diff_ADV,constants.Param_diff_ADP,\n",
    "        constants.Param_diff_CONJ,constants.Param_diff_PUNCT,constants.Param_diff_DET,constants.Param_diff_PRON,constants.Param_diff_AUX,constants.Param_diff_SYM,\n",
    "        constants.Param_diff_PART,\n",
    "        constants.Param_diff_countAbnormlWord,constants.Param_diff_totalNumberOfTokens,constants.Param_diff_ratioAbnormal\n",
    "        ]\n",
    "    dfTime=odf[columns].copy()\n",
    "    return dfTime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing ....\n",
      "\n",
      " Create dummies with OneHot encoder for subjects and modalities\n",
      "done \n",
      " Replace nan string with empty in aligned_edit and mt_text\n",
      "done \n",
      " Creating new columns number of words and number of uniaue words for src_text, mt_text and tgt_text\n",
      "\n",
      " Creating mask time data frame ...\n"
     ]
    }
   ],
   "source": [
    "new_dfTime=createMaskTimeDataFrame(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dfTime = new_dfTime.replace(np.NINF,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t1                          1.000000\n",
       "t2                          0.000000\n",
       "t3                          0.000000\n",
       "ht                          1.000000\n",
       "pe1                         0.000000\n",
       "                              ...   \n",
       "diff_SYM                    0.000000\n",
       "diff_PART                   0.000000\n",
       "diff_countAbnormlWord      -1.000000\n",
       "diff_totalNumberOfTokens    0.256410\n",
       "diff_ratioAbnormal         -1.689655\n",
       "Name: 0, Length: 84, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dfTime.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       149.979996\n",
       "1       128.078995\n",
       "2       141.500000\n",
       "3       151.317993\n",
       "4        66.817001\n",
       "           ...    \n",
       "1165     61.193001\n",
       "1166    143.360001\n",
       "1167    154.690002\n",
       "1168    182.785995\n",
       "1169    286.171997\n",
       "Name: edit_time, Length: 1170, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_edit_time = new_df[\"edit_time\"]\n",
    "label_edit_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Splitting the data set into 80% training and  20%test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_et,x_test_et,y_train_et,y_test_et = train_test_split(new_dfTime, label_edit_time, test_size = 0.2, random_state = 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((936, 84), (234, 84), (936,), (234,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_et.shape, x_test_et.shape,y_train_et.shape,y_test_et.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Random Forest regressor with hypertuned pararmeters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [i for i in range(4,20)],\n",
    "    'min_samples_leaf': [2,3, 4, 5],\n",
    "    'min_samples_split': [i for i in (range(2,8))],\n",
    "    'n_estimators': [i for i in range(10,40)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_et_tuning = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using K-Fold using 10 splits for hyper-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=10,random_state=None,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_et = GridSearchCV(estimator = RF_et_tuning, param_grid = param_grid, \n",
    "                          cv = cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=10, random_state=None, shuffle=False),\n",
       "             estimator=RandomForestRegressor(),\n",
       "             param_grid={'bootstrap': [True],\n",
       "                         'max_depth': [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n",
       "                                       16, 17, 18, 19],\n",
       "                         'min_samples_leaf': [2, 3, 4, 5],\n",
       "                         'min_samples_split': [2, 3, 4, 5, 6, 7],\n",
       "                         'n_estimators': [10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "                                          19, 20, 21, 22, 23, 24, 25, 26, 27,\n",
       "                                          28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
       "                                          37, 38, 39]})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_et.fit(x_train_et, y_train_et)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'max_depth': 9, 'min_samples_leaf': 3, 'min_samples_split': 7, 'n_estimators': 20}\n"
     ]
    }
   ],
   "source": [
    "print(grid_search_et.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor(max_depth=9, min_samples_leaf=3, min_samples_split=7,\n",
      "                      n_estimators=20)\n"
     ]
    }
   ],
   "source": [
    "print(grid_search_et.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(234,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_et_tuning = grid_search_et.predict(x_test_et)\n",
    "y_pred_et_tuning.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of the model using root mean square error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE 80.03425347327406\n"
     ]
    }
   ],
   "source": [
    "rmse_et = (np.sqrt(mean_squared_error(y_test_et,y_pred_et_tuning)))\n",
    "print(\"RMSE\",rmse_et)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance of the edit time prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable: k_total              Importance: 0.47\n",
      "Variable: k_cut                Importance: 0.04\n",
      "Variable: k_nav                Importance: 0.03\n",
      "Variable: tgt_ratioAbnormal    Importance: 0.03\n",
      "Variable: k_letter             Importance: 0.02\n",
      "Variable: k_symbol             Importance: 0.02\n",
      "Variable: num_unique_tgt_t_wise Importance: 0.02\n",
      "Variable: diff_ADJ             Importance: 0.02\n",
      "Variable: t2                   Importance: 0.01\n",
      "Variable: k_copy               Importance: 0.01\n",
      "Variable: k_erase              Importance: 0.01\n",
      "Variable: k_white              Importance: 0.01\n",
      "Variable: n_substitute         Importance: 0.01\n",
      "Variable: bleu                 Importance: 0.01\n",
      "Variable: chrf                 Importance: 0.01\n",
      "Variable: num_word_tgt         Importance: 0.01\n",
      "Variable: num_word_unique_tgt  Importance: 0.01\n",
      "Variable: num_of_S             Importance: 0.01\n",
      "Variable: num_of_IDS           Importance: 0.01\n",
      "Variable: src_NOUN             Importance: 0.01\n",
      "Variable: src_ADJ              Importance: 0.01\n",
      "Variable: src_DET              Importance: 0.01\n",
      "Variable: src_totalNumberOfTokens Importance: 0.01\n",
      "Variable: src_ratioAbnormal    Importance: 0.01\n",
      "Variable: tgt_NOUN             Importance: 0.01\n",
      "Variable: tgt_VERB             Importance: 0.01\n",
      "Variable: tgt_ADJ              Importance: 0.01\n",
      "Variable: tgt_ADV              Importance: 0.01\n",
      "Variable: tgt_ADP              Importance: 0.01\n",
      "Variable: tgt_countAbnormlWord Importance: 0.01\n",
      "Variable: tgt_totalNumberOfTokens Importance: 0.01\n",
      "Variable: diff_NOUN            Importance: 0.01\n",
      "Variable: diff_PROPN           Importance: 0.01\n",
      "Variable: diff_ADV             Importance: 0.01\n",
      "Variable: diff_ADP             Importance: 0.01\n",
      "Variable: diff_PUNCT           Importance: 0.01\n",
      "Variable: diff_DET             Importance: 0.01\n",
      "Variable: diff_countAbnormlWord Importance: 0.01\n",
      "Variable: diff_totalNumberOfTokens Importance: 0.01\n",
      "Variable: t1                   Importance: 0.0\n",
      "Variable: t3                   Importance: 0.0\n",
      "Variable: ht                   Importance: 0.0\n",
      "Variable: pe1                  Importance: 0.0\n",
      "Variable: pe2                  Importance: 0.0\n",
      "Variable: k_digit              Importance: 0.0\n",
      "Variable: k_paste              Importance: 0.0\n",
      "Variable: n_delete             Importance: 0.0\n",
      "Variable: n_insert             Importance: 0.0\n",
      "Variable: n_shift              Importance: 0.0\n",
      "Variable: num_annotations      Importance: 0.0\n",
      "Variable: ter                  Importance: 0.0\n",
      "Variable: num_word_mt          Importance: 0.0\n",
      "Variable: num_word_unique_mt   Importance: 0.0\n",
      "Variable: num_unique_mt_t_wise Importance: 0.0\n",
      "Variable: num_unique_mt_m_wise Importance: 0.0\n",
      "Variable: num_unique_tgt_m_wise Importance: 0.0\n",
      "Variable: num_of_I             Importance: 0.0\n",
      "Variable: num_of_D             Importance: 0.0\n",
      "Variable: src_PROPN            Importance: 0.0\n",
      "Variable: src_VERB             Importance: 0.0\n",
      "Variable: src_ADV              Importance: 0.0\n",
      "Variable: src_ADP              Importance: 0.0\n",
      "Variable: src_CONJ             Importance: 0.0\n",
      "Variable: src_PUNCT            Importance: 0.0\n",
      "Variable: src_PRON             Importance: 0.0\n",
      "Variable: src_AUX              Importance: 0.0\n",
      "Variable: src_SYM              Importance: 0.0\n",
      "Variable: src_PART             Importance: 0.0\n",
      "Variable: src_countAbnormlWord Importance: 0.0\n",
      "Variable: tgt_PROPN            Importance: 0.0\n",
      "Variable: tgt_CONJ             Importance: 0.0\n",
      "Variable: tgt_PUNCT            Importance: 0.0\n",
      "Variable: tgt_DET              Importance: 0.0\n",
      "Variable: tgt_PRON             Importance: 0.0\n",
      "Variable: tgt_AUX              Importance: 0.0\n",
      "Variable: tgt_SYM              Importance: 0.0\n",
      "Variable: tgt_PART             Importance: 0.0\n",
      "Variable: diff_VERB            Importance: 0.0\n",
      "Variable: diff_CONJ            Importance: 0.0\n",
      "Variable: diff_PRON            Importance: 0.0\n",
      "Variable: diff_AUX             Importance: 0.0\n",
      "Variable: diff_SYM             Importance: 0.0\n",
      "Variable: diff_PART            Importance: 0.0\n",
      "Variable: diff_ratioAbnormal   Importance: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Get numerical feature importances\n",
    "importances = list(grid_search_et.best_estimator_.feature_importances_)# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(new_dfTime.columns, importances)]# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model with mask edit time test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration mask_time-c8d896e19c9bb48e\n",
      "Reusing dataset ik_nlp22_pe_style (C:\\Users\\amit\\.cache\\huggingface\\datasets\\GroNLP___ik_nlp22_pe_style\\mask_time-c8d896e19c9bb48e\\1.0.0\\3bbf0fda4806257149c2beb42c6cd20db6f79dac9ae2498f44be55fa7a953d51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f3eaeccdc2148559808a3c084b44858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataDir='dataset/IK_NLP_22_PESTYLE'\n",
    "dataset_test_time = load_dataset(\"GroNLP/ik-nlp-22_pestyle\", \"mask_time\", data_dir=dataDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration mask_modality-c8d896e19c9bb48e\n",
      "Reusing dataset ik_nlp22_pe_style (C:\\Users\\amit\\.cache\\huggingface\\datasets\\GroNLP___ik_nlp22_pe_style\\mask_modality-c8d896e19c9bb48e\\1.0.0\\3bbf0fda4806257149c2beb42c6cd20db6f79dac9ae2498f44be55fa7a953d51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf76308edd64b51a7113a9648becb80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_test_modality = load_dataset(\"GroNLP/ik-nlp-22_pestyle\", \"mask_modality\", data_dir=dataDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mask edit time test dataset pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing ....\n",
      "\n",
      " Create dummies with OneHot encoder for subjects and modalities\n",
      "done \n",
      " Replace nan string with empty in aligned_edit and mt_text\n",
      "done \n",
      " Creating new columns number of words and number of uniaue words for src_text, mt_text and tgt_text\n",
      "\n",
      " Creating mask time data frame ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(120, 84)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_time = dataset_test_time[\"test\"]\n",
    "test_time_df = pd.DataFrame(data=test_time)\n",
    "test_time_pre_df=createMaskTimeDataFrame(test_time_df)\n",
    "test_time_pre_df = test_time_pre_df.replace(np.NINF,0)\n",
    "test_time_pre_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction from mask edit time test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytest_pred_et = grid_search_et.predict(test_time_pre_df)\n",
    "ytest_pred_et.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtaining the true edit time from masked modality test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 20)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_modality = dataset_test_modality[\"test\"]\n",
    "test_modality_df = pd.DataFrame(data=test_modality)\n",
    "test_modality_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_edit_times=test_modality_df[\"edit_time\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of the model on mask edit time test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE 59.08352459614063\n"
     ]
    }
   ],
   "source": [
    "rmse_et = (np.sqrt(mean_squared_error(test_edit_times,ytest_pred_et)))\n",
    "print(\"RMSE\",rmse_et)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
